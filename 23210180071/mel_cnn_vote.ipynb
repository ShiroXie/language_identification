{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[Learn the Basics](intro.html) ||\n",
        "**Quickstart** ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "[Build Model](buildmodel_tutorial.html) ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Quickstart\n",
        "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
        "\n",
        "## Working with data\n",
        "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
        "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
        "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
        "the ``Dataset``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split # 划分数据集\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "language_0 label: 0\n",
            "language_1 label: 1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data_path = \"./data/train_data\"\n",
        "test_data_path = \"./data/test_data\"\n",
        "\n",
        "def split_long_mel(mel_data, clip_target):\n",
        "    \"\"\"Split long mel spectrogram into clips with length clip_target\n",
        "    Args:\n",
        "        mel_data: mel spectrogram data\n",
        "        clip_target: target clip length\n",
        "    Returns:\n",
        "        tmp_data: list of mel spectrogram clips\n",
        "    \"\"\"\n",
        "    tmp_data = []\n",
        "    if mel_data.shape[0] > clip_target:\n",
        "        n = len(mel_data)\n",
        "        split_indices = list(range(clip_target, n, clip_target))  \n",
        "        splited_array = np.split(mel_data, split_indices) \n",
        "        if splited_array[-1].shape[0] <= clip_target // 2:\n",
        "            splited_array[-1] = mel_data[-clip_target // 2:]\n",
        "        splited_array[-1] = np.pad(splited_array[-1], ((0, clip_target - splited_array[-1].shape[0]), (0, 0)), mode=\"constant\")\n",
        "        tmp_data.extend(splited_array)\n",
        "    else:\n",
        "        mel_data = np.pad(mel_data, ((0, clip_target - mel_data.shape[0]), (0, 0)), mode=\"constant\")\n",
        "        tmp_data.append(mel_data)\n",
        "    return tmp_data\n",
        "\n",
        "X_0 = []\n",
        "Y_0 = []\n",
        "X_1 = []\n",
        "Y_1 = []\n",
        "mel_len = []\n",
        "clip_target = 256 # train: max 734, min 120, mean 293, median 281\n",
        "# test: 766 40 190.651 199.0\n",
        "\n",
        "\n",
        "# load train data\n",
        "for directory in os.listdir(train_data_path):\n",
        "    label = int(directory.split(\"_\")[1])\n",
        "    print(directory, \"label:\", label)\n",
        "    for file in os.listdir(os.path.join(train_data_path, directory)):\n",
        "        mel_data = np.load(os.path.join(train_data_path, directory, file))\n",
        "        mel_len.append(mel_data.shape[0])\n",
        "        tmp_data = split_long_mel(mel_data, clip_target)\n",
        "        tmp_label = [label] * len(tmp_data)\n",
        "        if label == 0:\n",
        "            X_0.extend(tmp_data)\n",
        "            Y_0.extend(tmp_label)\n",
        "        else:\n",
        "            X_1.extend(tmp_data)\n",
        "            Y_1.extend(tmp_label)\n",
        "# visualize the distribution of raw mel length\n",
        "mel_len = np.array(mel_len)\n",
        "plt.hist(mel_len, bins=10, alpha=0.5, label='Data')\n",
        "plt.show()\n",
        "X_0 = np.array(X_0)\n",
        "Y_0 = np.array(Y_0)\n",
        "X_1 = np.array(X_1)\n",
        "Y_1 = np.array(Y_1)\n",
        "\n",
        "# split train, valid and fake test data\n",
        "X_0_train, X_0_valid, y_0_train, y_0_valid = train_test_split(X_0, Y_0, test_size=0.1, random_state=42)\n",
        "X_1_train, X_1_valid, y_1_train, y_1_valid = train_test_split(X_1, Y_1, test_size=0.1, random_state=42)\n",
        "# split the fake test data from valid data to avoid overfitting\n",
        "X_0_fake_test, X_0_valid, y_0_fake_test, y_0_valid = train_test_split(X_0_valid, y_0_valid, test_size=0.5, random_state=42)\n",
        "X_1_fake_test, X_1_valid, y_1_fake_test, y_1_valid = train_test_split(X_1_valid, y_1_valid, test_size=0.5, random_state=42)\n",
        "X_fake_test = np.concatenate((X_0_fake_test, X_1_fake_test), axis=0)\n",
        "y_fake_test = np.concatenate((y_0_fake_test, y_1_fake_test), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "X_train = np.concatenate((X_0_train, X_1_train), axis=0)\n",
        "y_train = np.concatenate((y_0_train, y_1_train), axis=0)\n",
        "X_valid = np.concatenate((X_0_valid, X_1_valid), axis=0)\n",
        "y_valid = np.concatenate((y_0_valid, y_1_valid), axis=0)\n",
        "\n",
        "# prepare data loader\n",
        "training_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
        "valid_data = TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid).float())\n",
        "fake_test_data = TensorDataset(torch.from_numpy(X_fake_test).float(), torch.from_numpy(y_fake_test).float())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([128, 1, 256, 80])\n",
            "Shape of y: torch.Size([128]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "def random_mask_collate_fn(batch):\n",
        "    \"\"\"Add random mask to the input image\n",
        "    Args:\n",
        "        batch: list of (image, label)\n",
        "    Returns:\n",
        "        masked_images: tensor of masked images\n",
        "        labels: tensor of labels\n",
        "    \"\"\"\n",
        "    images, labels = zip(*batch)\n",
        "    masks = []\n",
        "    for image in images:\n",
        "        mask = torch.ones_like(image)\n",
        "        mask_height, mask_width = mask.shape[0], mask.shape[1]\n",
        "        if np.random.rand() < 0.5:\n",
        "            # mask area is a horizontal line\n",
        "            mask_area_width = random.randint(0, mask_width // 4)\n",
        "            mask_width_start = random.randint(0, mask_width - mask_area_width)\n",
        "            mask[:, mask_width_start:mask_width_start+mask_area_width] = 0\n",
        "        else:\n",
        "            # mask area is a vertical line\n",
        "            mask_area_height = random.randint(0, mask_height // 5)\n",
        "            mask_height_start = random.randint(0, mask_height - mask_area_height)\n",
        "            mask[mask_height_start:mask_height_start+mask_area_height, :] = 0     \n",
        "        masks.append(mask)\n",
        "    masked_images = [image * mask for image, mask in zip(images, masks)]\n",
        "    masked_images = torch.stack(masked_images)\n",
        "    masks = torch.stack(masks)\n",
        "    labels = torch.tensor(labels)\n",
        "    labels = labels.long()\n",
        "    masked_images = masked_images.unsqueeze(1)\n",
        "    return masked_images, labels\n",
        "\n",
        "def valid_collate_fn(batch):\n",
        "    \"\"\"collect batch of images and labels for validation and test\n",
        "    Args:\n",
        "        batch: list of (image, label)\n",
        "    Returns:\n",
        "        images: tensor of images\n",
        "        labels: tensor of labels\n",
        "    \"\"\"\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "    labels = labels.long()\n",
        "    images = images.unsqueeze(1)\n",
        "    return images, labels\n",
        "    \n",
        "# define data loaders\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=random_mask_collate_fn)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=valid_collate_fn)\n",
        "test_data_loader = DataLoader(fake_test_data, batch_size=batch_size, shuffle=True, collate_fn=valid_collate_fn)\n",
        "\n",
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU or MPS if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device\n",
            "NeuralNetwork(\n",
            "  (CBLs): Sequential(\n",
            "    (0): CBL(\n",
            "      (conv): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): CBL(\n",
            "      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): CBL(\n",
            "      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Dropout(p=0.4, inplace=False)\n",
            "    (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3248, out_features=128, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "class CBL(nn.Module):\n",
        "    \"\"\"Convolution + Batch norm + LeakyReLU\n",
        "    Args:\n",
        "        in_channels: input channels\n",
        "        out_channels: output channels\n",
        "        kernel_size: kernel size\n",
        "        stride: stride\n",
        "        padding: padding\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(CBL, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "    \n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"Define neural network\n",
        "    Structure:\n",
        "        CBL(1, 16, 7, 1, 0)\n",
        "        MaxPool2d(2, 2)\n",
        "        CBL(16, 16, 5, 1, 0)\n",
        "        MaxPool2d(2, 2)\n",
        "        CBL(16, 16, 3, 1, 0)\n",
        "        MaxPool2d(2, 2)\n",
        "        Dropout(0.4)\n",
        "        Flatten()\n",
        "        Linear(3248, 128)\n",
        "        LeakyReLU()\n",
        "        Linear(128, 2)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.CBLs = nn.Sequential(\n",
        "            CBL(1, 16, 7, 1, 0),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            CBL(16, 16, 5, 1, 0),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            CBL(16, 16, 3, 1, 0),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3248, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.CBLs(x)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [building neural networks in PyTorch](buildmodel_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    \"\"\"function for training\n",
        "    Args:\n",
        "        dataloader: data loader\n",
        "        model: neural network model\n",
        "        loss_fn: loss function\n",
        "        optimizer: optimizer\n",
        "    Returns:\n",
        "        loss_mean: mean loss of this epoch\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "    loss_mean = np.mean(losses)\n",
        "    print(f\"Training loss: {loss_mean:.4f}\")\n",
        "    return loss_mean\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    \"\"\"function for testing\n",
        "    Args:\n",
        "        dataloader: data loader\n",
        "        model: neural network model\n",
        "        loss_fn: loss function\n",
        "    Returns:\n",
        "        correct: accuracy of this epoch\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loss: 0.7036\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.680733 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.9%, Avg loss: 0.699081 \n",
            "\n",
            "Save model: acc 0.5775075987841946, test acc 0.47865853658536583, loss 0.7035780472958342\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loss: 0.6906\n",
            "Test Error: \n",
            " Accuracy: 58.7%, Avg loss: 0.670405 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.1%, Avg loss: 0.691972 \n",
            "\n",
            "Save model: acc 0.5866261398176292, test acc 0.5213414634146342, loss 0.6906359943937748\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loss: 0.6859\n",
            "Test Error: \n",
            " Accuracy: 61.7%, Avg loss: 0.668079 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.0%, Avg loss: 0.686922 \n",
            "\n",
            "Save model: acc 0.6170212765957447, test acc 0.5304878048780488, loss 0.6859210501325891\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loss: 0.6812\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 0.662600 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.1%, Avg loss: 0.684786 \n",
            "\n",
            "Save model: acc 0.6291793313069909, test acc 0.5609756097560976, loss 0.6811724203698178\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loss: 0.6785\n",
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 0.653967 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.9%, Avg loss: 0.677224 \n",
            "\n",
            "Save model: acc 0.6352583586626139, test acc 0.5792682926829268, loss 0.6784981387726804\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loss: 0.6693\n",
            "Test Error: \n",
            " Accuracy: 65.0%, Avg loss: 0.645782 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 0.669649 \n",
            "\n",
            "Save model: acc 0.6504559270516718, test acc 0.5914634146341463, loss 0.6692565831732242\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loss: 0.6665\n",
            "Test Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.640580 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.668059 \n",
            "\n",
            "Save model: acc 0.6656534954407295, test acc 0.6341463414634146, loss 0.6664668169427426\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loss: 0.6603\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 0.634611 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.663372 \n",
            "\n",
            "Save model: acc 0.6778115501519757, test acc 0.6310975609756098, loss 0.660261775584931\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loss: 0.6589\n",
            "Test Error: \n",
            " Accuracy: 68.7%, Avg loss: 0.627487 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.659011 \n",
            "\n",
            "Save model: acc 0.6869300911854104, test acc 0.6585365853658537, loss 0.6588945439521302\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loss: 0.6527\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.625999 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.650207 \n",
            "\n",
            "Save model: acc 0.7082066869300911, test acc 0.6341463414634146, loss 0.6526606767735583\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loss: 0.6500\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.616054 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.647934 \n",
            "\n",
            "Save model: acc 0.7112462006079028, test acc 0.676829268292683, loss 0.6499643300441985\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loss: 0.6435\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.608221 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.637981 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loss: 0.6399\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.602578 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.635174 \n",
            "\n",
            "Save model: acc 0.7173252279635258, test acc 0.6798780487804879, loss 0.6399060513110871\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loss: 0.6360\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.594891 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.622271 \n",
            "\n",
            "Save model: acc 0.7203647416413373, test acc 0.6798780487804879, loss 0.636008362820808\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loss: 0.6267\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.589044 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.620276 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loss: 0.6211\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.584074 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.615016 \n",
            "\n",
            "Save model: acc 0.7355623100303952, test acc 0.698170731707317, loss 0.621054211829571\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loss: 0.6163\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.572065 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.3%, Avg loss: 0.604973 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loss: 0.6095\n",
            "Test Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.568225 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.3%, Avg loss: 0.596442 \n",
            "\n",
            "Save model: acc 0.7689969604863222, test acc 0.7134146341463414, loss 0.6095374112433576\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loss: 0.6032\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.557635 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.6%, Avg loss: 0.589968 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loss: 0.5968\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.544463 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.586310 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loss: 0.5893\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.532644 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.573320 \n",
            "\n",
            "Save model: acc 0.7720364741641338, test acc 0.7347560975609756, loss 0.5893310447956653\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loss: 0.5890\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.527105 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.561703 \n",
            "\n",
            "Save model: acc 0.7963525835866262, test acc 0.7378048780487805, loss 0.5890481611515613\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loss: 0.5786\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.521712 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.552978 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loss: 0.5703\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.507221 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.534290 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loss: 0.5618\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.506306 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.534922 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loss: 0.5523\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.493265 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.523661 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loss: 0.5547\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.482850 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.525694 \n",
            "\n",
            "Save model: acc 0.817629179331307, test acc 0.7621951219512195, loss 0.5547094763593471\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loss: 0.5439\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.466523 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.505171 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loss: 0.5323\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.475248 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.503915 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loss: 0.5315\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.457952 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.483608 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loss: 0.5234\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.451004 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.479711 \n",
            "\n",
            "Save model: acc 0.8297872340425532, test acc 0.7774390243902439, loss 0.5233853966631787\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loss: 0.5110\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.437772 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.473593 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loss: 0.5092\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.436312 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.479732 \n",
            "\n",
            "Save model: acc 0.8328267477203647, test acc 0.7835365853658537, loss 0.5091692145834578\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loss: 0.4959\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.427894 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.455959 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loss: 0.4949\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.417974 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.451578 \n",
            "\n",
            "Save model: acc 0.8358662613981763, test acc 0.7896341463414634, loss 0.49489343039532924\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loss: 0.4886\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.414598 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.448480 \n",
            "\n",
            "Save model: acc 0.8480243161094225, test acc 0.7804878048780488, loss 0.488569697801103\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loss: 0.4827\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.402376 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.446436 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loss: 0.4815\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.394461 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.425402 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loss: 0.4695\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.389478 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.424385 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loss: 0.4674\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.382693 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.421034 \n",
            "\n",
            "Save model: acc 0.8632218844984803, test acc 0.8170731707317073, loss 0.46735800897821467\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loss: 0.4575\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.381103 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.409256 \n",
            "\n",
            "Save model: acc 0.8662613981762918, test acc 0.7957317073170732, loss 0.45750766865750575\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loss: 0.4499\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.379901 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.408601 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loss: 0.4509\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.378777 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.405095 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loss: 0.4418\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.358284 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.407159 \n",
            "\n",
            "Save model: acc 0.8723404255319149, test acc 0.8170731707317073, loss 0.4418380875536736\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loss: 0.4344\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.353074 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.378676 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loss: 0.4316\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.358008 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.388967 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loss: 0.4268\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.351316 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.384709 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loss: 0.4281\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.341397 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.369322 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loss: 0.4287\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.329024 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.358428 \n",
            "\n",
            "Save model: acc 0.878419452887538, test acc 0.8323170731707317, loss 0.4286840361483554\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loss: 0.4170\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.349773 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.362562 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loss: 0.4122\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.325932 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.364046 \n",
            "\n",
            "Save model: acc 0.8814589665653495, test acc 0.8262195121951219, loss 0.4121664952724538\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loss: 0.4025\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.311001 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.355490 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loss: 0.3977\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.319372 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.356707 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loss: 0.3865\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.303882 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.338703 \n",
            "\n",
            "Save model: acc 0.8936170212765957, test acc 0.8567073170731707, loss 0.3865228124121402\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loss: 0.3868\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.308753 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.365007 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loss: 0.3777\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.315738 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.337640 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loss: 0.3732\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.294757 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.323851 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loss: 0.3759\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.327866 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.360034 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loss: 0.3699\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.313254 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.329667 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loss: 0.3597\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.271674 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.303307 \n",
            "\n",
            "Save model: acc 0.8996960486322189, test acc 0.8780487804878049, loss 0.35968370291780916\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loss: 0.3556\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.276512 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.305634 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loss: 0.3575\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.374081 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.369460 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loss: 0.3484\n",
            "Test Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.438665 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.416765 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loss: 0.3395\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.265600 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.290386 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loss: 0.3359\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.619981 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.605505 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loss: 0.3373\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.294699 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.294093 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loss: 0.3249\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.240455 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.277551 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loss: 0.3226\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 0.610964 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 0.625057 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loss: 0.3214\n",
            "Test Error: \n",
            " Accuracy: 57.1%, Avg loss: 0.885924 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 0.865937 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loss: 0.3194\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.319082 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.367485 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loss: 0.3157\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.594125 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.592005 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loss: 0.3130\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.295663 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.289954 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loss: 0.3105\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.281035 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.289517 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loss: 0.3068\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.285995 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.318187 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loss: 0.2987\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.229979 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.270501 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loss: 0.2941\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.342695 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.339799 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loss: 0.2941\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.339181 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.335366 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loss: 0.2924\n",
            "Test Error: \n",
            " Accuracy: 52.6%, Avg loss: 1.388530 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.0%, Avg loss: 1.569505 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loss: 0.2943\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.651296 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.775387 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loss: 0.2827\n",
            "Test Error: \n",
            " Accuracy: 53.5%, Avg loss: 1.644064 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.0%, Avg loss: 1.539694 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loss: 0.2793\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.187557 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.228381 \n",
            "\n",
            "Save model: acc 0.939209726443769, test acc 0.9054878048780488, loss 0.2793368789109778\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loss: 0.2872\n",
            "Test Error: \n",
            " Accuracy: 77.8%, Avg loss: 0.432701 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.399811 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loss: 0.2725\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.416314 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.454235 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loss: 0.2798\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.471700 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.465232 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loss: 0.2687\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.664187 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.651479 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loss: 0.2647\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.257984 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.257622 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loss: 0.2642\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.198958 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.238737 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loss: 0.2639\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 0.970779 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 0.919890 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loss: 0.2579\n",
            "Test Error: \n",
            " Accuracy: 55.6%, Avg loss: 1.289698 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 1.415825 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loss: 0.2595\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.305266 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.341403 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loss: 0.2547\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.220360 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.219334 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loss: 0.2541\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 0.819969 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.2%, Avg loss: 0.815100 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loss: 0.2600\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 5.921985 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 5.765391 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loss: 0.2552\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 1.243474 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.8%, Avg loss: 1.157179 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loss: 0.2565\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 5.965834 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 5.989119 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loss: 0.2521\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.205084 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.225246 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loss: 0.2421\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.181119 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.178771 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loss: 0.2377\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 2.573371 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 2.464815 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loss: 0.2430\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.447367 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.493982 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loss: 0.2307\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.194043 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.181890 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loss: 0.2376\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 1.737798 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 1.923303 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loss: 0.2352\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.347028 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.395002 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loss: 0.2227\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.266881 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.259023 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loss: 0.2248\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.165148 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.168939 \n",
            "\n",
            "Save model: acc 0.9483282674772037, test acc 0.9237804878048781, loss 0.22475227967221686\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loss: 0.2306\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.337968 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 0.332721 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loss: 0.2254\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.150424 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.167060 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loss: 0.2206\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.783132 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 4.710852 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loss: 0.2281\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 2.836301 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 2.650785 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loss: 0.2180\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 3.104876 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 3.326927 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loss: 0.2221\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.305968 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.335172 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loss: 0.2148\n",
            "Test Error: \n",
            " Accuracy: 52.6%, Avg loss: 2.100358 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 2.220342 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loss: 0.2149\n",
            "Test Error: \n",
            " Accuracy: 56.2%, Avg loss: 1.341504 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.1%, Avg loss: 1.271748 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loss: 0.2186\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.527159 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.587722 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loss: 0.2076\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.273779 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.256622 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loss: 0.2151\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 4.232382 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 4.464523 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loss: 0.2101\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.390699 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.376623 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loss: 0.2035\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.135597 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.153113 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loss: 0.2119\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.603515 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.570729 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loss: 0.2082\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 2.493522 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.7%, Avg loss: 2.700205 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loss: 0.2049\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.228581 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.223189 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loss: 0.2108\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 4.389215 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 4.636920 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loss: 0.1961\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 2.983216 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 2.825114 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loss: 0.2013\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.840162 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 1.062762 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loss: 0.2002\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.373816 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.438493 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loss: 0.1959\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.155621 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.173784 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loss: 0.1946\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.162368 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.157483 \n",
            "\n",
            "Save model: acc 0.9513677811550152, test acc 0.9420731707317073, loss 0.194619025004671\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loss: 0.1904\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.129841 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.136575 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loss: 0.1894\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 5.844699 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 6.022658 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loss: 0.1948\n",
            "Test Error: \n",
            " Accuracy: 66.3%, Avg loss: 0.799212 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.767437 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loss: 0.1907\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.500858 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.566180 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loss: 0.1917\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 3.700093 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 3.599159 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loss: 0.1917\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.128735 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.124286 \n",
            "\n",
            "Save model: acc 0.9544072948328267, test acc 0.948170731707317, loss 0.1917342760778488\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loss: 0.1928\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.308431 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.297069 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loss: 0.1828\n",
            "Test Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.845682 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.731424 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loss: 0.1800\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.239021 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.278619 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loss: 0.1801\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 2.247328 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.0%, Avg loss: 2.154273 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loss: 0.1838\n",
            "Test Error: \n",
            " Accuracy: 53.5%, Avg loss: 1.876566 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.9%, Avg loss: 1.807968 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loss: 0.1792\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.350819 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.407208 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loss: 0.1814\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.111262 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 3.854290 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loss: 0.1774\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.816884 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.952616 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loss: 0.1781\n",
            "Test Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.961643 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 1.110631 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loss: 0.1786\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.150951 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.121934 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loss: 0.1746\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.795804 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.860185 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loss: 0.1695\n",
            "Test Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.557854 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.606470 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loss: 0.1680\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.129390 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.107210 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loss: 0.1700\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.141042 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.108498 \n",
            "\n",
            "Save model: acc 0.9574468085106383, test acc 0.9603658536585366, loss 0.17001872763354728\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loss: 0.1740\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 0.909381 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 1.024814 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loss: 0.1710\n",
            "Test Error: \n",
            " Accuracy: 53.2%, Avg loss: 2.469981 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.3%, Avg loss: 2.302902 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loss: 0.1702\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.203911 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.224253 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loss: 0.1695\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 3.851385 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 3.741917 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loss: 0.1792\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 4.511517 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 4.622029 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loss: 0.1682\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.710757 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.827513 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loss: 0.1693\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.328924 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.375405 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loss: 0.1772\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 4.267048 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 4.722409 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loss: 0.1654\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.483515 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 4.118148 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loss: 0.1800\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 9.418338 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 9.972706 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loss: 0.1807\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.714423 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.643821 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loss: 0.1643\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.224763 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.2%, Avg loss: 1.182356 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loss: 0.1669\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.326249 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.304149 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loss: 0.1679\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 3.017310 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.3%, Avg loss: 3.154337 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loss: 0.1673\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 3.367489 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.1%, Avg loss: 3.524396 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loss: 0.1532\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.109339 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.100971 \n",
            "\n",
            "Save model: acc 0.9665653495440729, test acc 0.9695121951219512, loss 0.15323525924790413\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loss: 0.1563\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.221409 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.189842 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loss: 0.1572\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.484158 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.520000 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loss: 0.1581\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.173887 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.180212 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loss: 0.1631\n",
            "Test Error: \n",
            " Accuracy: 56.5%, Avg loss: 1.973192 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 2.141375 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loss: 0.1653\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.776946 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.697212 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loss: 0.1553\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.555491 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.474155 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loss: 0.1523\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.110952 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.095234 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loss: 0.1546\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.098440 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.081550 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loss: 0.1537\n",
            "Test Error: \n",
            " Accuracy: 52.6%, Avg loss: 2.525657 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 2.875660 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loss: 0.1445\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.149178 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.130003 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loss: 0.1469\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 3.122210 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 3.181292 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loss: 0.1498\n",
            "Test Error: \n",
            " Accuracy: 79.3%, Avg loss: 0.481380 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.400728 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loss: 0.1504\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.575417 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.659144 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loss: 0.1535\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 2.895471 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.7%, Avg loss: 3.055979 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loss: 0.1531\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.138409 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 1.080086 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loss: 0.1528\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 6.577729 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 6.349461 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loss: 0.1524\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.119882 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.163010 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loss: 0.1501\n",
            "Test Error: \n",
            " Accuracy: 58.4%, Avg loss: 1.949935 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.2%, Avg loss: 2.110452 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loss: 0.1498\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.179462 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.171152 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loss: 0.1463\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.925984 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.907090 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loss: 0.1484\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 3.583082 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 3.390535 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loss: 0.1452\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.124297 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.130713 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loss: 0.1436\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.242664 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.186986 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loss: 0.1477\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 2.960234 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.3%, Avg loss: 3.068768 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loss: 0.1465\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 5.522921 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 5.134364 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loss: 0.1406\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.456575 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.355125 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loss: 0.1476\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.206507 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.202595 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loss: 0.1417\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.555997 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 4.327144 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loss: 0.1439\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 3.347651 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 3.101173 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loss: 0.1421\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.338633 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.270181 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loss: 0.1450\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.131490 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.100235 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loss: 0.1420\n",
            "Test Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.928586 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.820498 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loss: 0.1354\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 3.322666 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 3.065502 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loss: 0.1362\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.140025 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.134804 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loss: 0.1455\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 7.003402 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 6.798043 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loss: 0.1400\n",
            "Test Error: \n",
            " Accuracy: 61.4%, Avg loss: 1.223059 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.085090 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loss: 0.1386\n",
            "Test Error: \n",
            " Accuracy: 57.4%, Avg loss: 1.775107 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.0%, Avg loss: 1.626565 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loss: 0.1562\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.643715 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.675469 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loss: 0.1343\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.813507 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.860529 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loss: 0.1306\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.144073 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.144049 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loss: 0.1311\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.200386 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.189820 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loss: 0.1337\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.110898 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.2%, Avg loss: 1.328947 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loss: 0.1392\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.292142 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.8%, Avg loss: 1.421115 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loss: 0.1425\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.151202 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.120703 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loss: 0.1361\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.813929 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.641561 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loss: 0.1313\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 1.212896 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 1.305149 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loss: 0.1339\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.170002 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.136824 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loss: 0.1300\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.712402 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.802921 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loss: 0.1325\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.251270 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.9%, Avg loss: 0.251043 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loss: 0.1345\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.290133 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.240044 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loss: 0.1303\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.162579 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.173300 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loss: 0.1297\n",
            "Test Error: \n",
            " Accuracy: 54.1%, Avg loss: 2.375727 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.2%, Avg loss: 2.178806 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loss: 0.1312\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 2.402194 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 2.162536 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loss: 0.1281\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 2.981686 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 2.998793 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loss: 0.1321\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.363485 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.470051 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loss: 0.1269\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.357431 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.303194 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loss: 0.1236\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.075523 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.952910 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loss: 0.1280\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.499877 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.485149 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loss: 0.1273\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.096169 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.068657 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loss: 0.1343\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.495248 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.562831 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loss: 0.1230\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.086872 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.064213 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loss: 0.1241\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.097879 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.072427 \n",
            "\n",
            "Save model: acc 0.9696048632218845, test acc 0.9695121951219512, loss 0.12407348907374322\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loss: 0.1312\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 3.748677 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 4.149367 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loss: 0.1339\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.117399 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.076539 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loss: 0.1229\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.265871 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.224222 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loss: 0.1310\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.110176 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.113103 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loss: 0.1273\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 8.174361 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 7.950186 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loss: 0.1198\n",
            "Test Error: \n",
            " Accuracy: 58.7%, Avg loss: 1.918051 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.6%, Avg loss: 2.075313 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loss: 0.1177\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.149706 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.127889 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loss: 0.1141\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.338501 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.298302 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loss: 0.1308\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 5.022415 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 4.734537 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loss: 0.1168\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.117804 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.110228 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loss: 0.1270\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.078218 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.069951 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loss: 0.1209\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.234095 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.202784 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loss: 0.1177\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.176381 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.173624 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loss: 0.1160\n",
            "Test Error: \n",
            " Accuracy: 49.2%, Avg loss: 4.472555 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 4.939185 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loss: 0.1175\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.276584 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 1.396118 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loss: 0.1229\n",
            "Test Error: \n",
            " Accuracy: 75.1%, Avg loss: 0.793904 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.609011 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loss: 0.1169\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.399716 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.374443 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loss: 0.1220\n",
            "Test Error: \n",
            " Accuracy: 54.1%, Avg loss: 2.203726 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.2%, Avg loss: 2.061201 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loss: 0.1176\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.314817 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.318257 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loss: 0.1176\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 2.092830 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.8%, Avg loss: 2.261156 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loss: 0.1208\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.088079 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.076762 \n",
            "\n",
            "Save model: acc 0.9726443768996961, test acc 0.9725609756097561, loss 0.120841679737923\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loss: 0.1077\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 1.371256 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.217383 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loss: 0.1196\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.943411 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.745493 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loss: 0.1145\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 0.906261 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.794602 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loss: 0.1220\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.969292 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 4.945243 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loss: 0.1241\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.239684 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.176084 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loss: 0.1181\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.198480 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.183177 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loss: 0.1196\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.406142 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.358906 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loss: 0.1128\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 4.523293 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 4.318100 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loss: 0.1150\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 6.074222 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 6.552804 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loss: 0.1155\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.203324 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.166538 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loss: 0.1039\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.080463 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.061716 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loss: 0.1120\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.212922 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.204536 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loss: 0.1102\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.078080 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.058357 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loss: 0.1119\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.308267 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.302245 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loss: 0.1101\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.262702 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.234633 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loss: 0.1098\n",
            "Test Error: \n",
            " Accuracy: 66.3%, Avg loss: 1.220895 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 1.240321 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loss: 0.1174\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.094953 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.076182 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loss: 0.1090\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.193732 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.197633 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loss: 0.1189\n",
            "Test Error: \n",
            " Accuracy: 52.0%, Avg loss: 5.024442 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.1%, Avg loss: 5.932530 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loss: 0.1220\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.115549 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.098509 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loss: 0.1194\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.150158 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.119581 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loss: 0.1025\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.329938 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.326038 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loss: 0.1048\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.095233 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.080723 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loss: 0.1063\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.111259 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.084870 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loss: 0.1188\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.560566 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.660699 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loss: 0.1140\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.101369 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.071536 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loss: 0.1066\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.089013 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.067254 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loss: 0.1062\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.094697 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.068709 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loss: 0.1018\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.699350 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.624890 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loss: 0.1100\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.114410 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.066737 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loss: 0.1054\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.299004 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.379592 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loss: 0.1083\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.089616 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.087555 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loss: 0.1063\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.097560 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.069026 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loss: 0.1086\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.102239 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.086839 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loss: 0.1023\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 2.583128 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.3%, Avg loss: 2.874643 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loss: 0.1055\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.097675 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.073735 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loss: 0.1104\n",
            "Test Error: \n",
            " Accuracy: 90.3%, Avg loss: 0.225891 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.233116 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loss: 0.1117\n",
            "Test Error: \n",
            " Accuracy: 57.1%, Avg loss: 1.946677 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 1.769820 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loss: 0.1038\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 2.356839 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 2.778571 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loss: 0.1022\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.137337 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.118924 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loss: 0.1033\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.412545 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.395356 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loss: 0.1029\n",
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 1.632780 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.371509 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loss: 0.1061\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 4.120312 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 3.949857 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loss: 0.1066\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.068041 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.063385 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loss: 0.1005\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.292197 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.298220 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loss: 0.0934\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 3.577207 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.7%, Avg loss: 3.300759 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loss: 0.1044\n",
            "Test Error: \n",
            " Accuracy: 55.3%, Avg loss: 2.263129 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.3%, Avg loss: 2.052029 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loss: 0.0990\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.928896 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.821887 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loss: 0.1097\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.097702 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.071802 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loss: 0.1058\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 7.625780 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 7.354841 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loss: 0.1060\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.147955 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.123621 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loss: 0.0979\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.410165 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.333803 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loss: 0.0955\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 1.423751 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.550961 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loss: 0.1061\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 1.220293 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 1.260810 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loss: 0.0959\n",
            "Test Error: \n",
            " Accuracy: 90.3%, Avg loss: 0.343410 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.293716 \n",
            "\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "Training loss: 0.1027\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.370266 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.313357 \n",
            "\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "Training loss: 0.1040\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.507103 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.419248 \n",
            "\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "Training loss: 0.0998\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 3.773475 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 3.454182 \n",
            "\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "Training loss: 0.1028\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 1.062345 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.945591 \n",
            "\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "Training loss: 0.0961\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.541168 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.581939 \n",
            "\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "Training loss: 0.1023\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.101815 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.071024 \n",
            "\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "Training loss: 0.0989\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.175282 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.136277 \n",
            "\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "Training loss: 0.0955\n",
            "Test Error: \n",
            " Accuracy: 57.1%, Avg loss: 1.915415 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 1.749994 \n",
            "\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "Training loss: 0.1006\n",
            "Test Error: \n",
            " Accuracy: 56.5%, Avg loss: 2.272428 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.1%, Avg loss: 2.089632 \n",
            "\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "Training loss: 0.0994\n",
            "Test Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.446298 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.432396 \n",
            "\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "Training loss: 0.1124\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.101283 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.072277 \n",
            "\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "Training loss: 0.1020\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.428759 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.349746 \n",
            "\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "Training loss: 0.0993\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.123295 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.114809 \n",
            "\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "Training loss: 0.0974\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.151002 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.134681 \n",
            "\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "Training loss: 0.0936\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.097210 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.065915 \n",
            "\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "Training loss: 0.0972\n",
            "Test Error: \n",
            " Accuracy: 52.6%, Avg loss: 3.227292 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.7%, Avg loss: 2.939694 \n",
            "\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "Training loss: 0.0891\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.803071 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.901212 \n",
            "\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "Training loss: 0.0897\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 5.052200 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 4.727379 \n",
            "\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "Training loss: 0.1183\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.182801 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.138242 \n",
            "\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "Training loss: 0.0937\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.373533 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.325999 \n",
            "\n",
            "Epoch 321\n",
            "-------------------------------\n",
            "Training loss: 0.0954\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.878136 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.821139 \n",
            "\n",
            "Epoch 322\n",
            "-------------------------------\n",
            "Training loss: 0.0955\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.114676 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.093995 \n",
            "\n",
            "Epoch 323\n",
            "-------------------------------\n",
            "Training loss: 0.1002\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.305870 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.247728 \n",
            "\n",
            "Epoch 324\n",
            "-------------------------------\n",
            "Training loss: 0.0932\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.223961 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.187137 \n",
            "\n",
            "Epoch 325\n",
            "-------------------------------\n",
            "Training loss: 0.0931\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.139853 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.098341 \n",
            "\n",
            "Epoch 326\n",
            "-------------------------------\n",
            "Training loss: 0.0993\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.257381 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.9%, Avg loss: 0.236858 \n",
            "\n",
            "Epoch 327\n",
            "-------------------------------\n",
            "Training loss: 0.0901\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.140066 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.145254 \n",
            "\n",
            "Epoch 328\n",
            "-------------------------------\n",
            "Training loss: 0.0956\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.072916 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.054915 \n",
            "\n",
            "Epoch 329\n",
            "-------------------------------\n",
            "Training loss: 0.0930\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.087424 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.057166 \n",
            "\n",
            "Epoch 330\n",
            "-------------------------------\n",
            "Training loss: 0.0942\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.124463 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.095028 \n",
            "\n",
            "Epoch 331\n",
            "-------------------------------\n",
            "Training loss: 0.0949\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 3.468833 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 3.191464 \n",
            "\n",
            "Epoch 332\n",
            "-------------------------------\n",
            "Training loss: 0.0915\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.119683 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.100211 \n",
            "\n",
            "Epoch 333\n",
            "-------------------------------\n",
            "Training loss: 0.0966\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.108540 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.058070 \n",
            "\n",
            "Epoch 334\n",
            "-------------------------------\n",
            "Training loss: 0.0937\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 6.108075 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 5.857654 \n",
            "\n",
            "Epoch 335\n",
            "-------------------------------\n",
            "Training loss: 0.0977\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.124154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.115976 \n",
            "\n",
            "Epoch 336\n",
            "-------------------------------\n",
            "Training loss: 0.0978\n",
            "Test Error: \n",
            " Accuracy: 52.0%, Avg loss: 5.051732 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 4.735722 \n",
            "\n",
            "Epoch 337\n",
            "-------------------------------\n",
            "Training loss: 0.0968\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.136253 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.106409 \n",
            "\n",
            "Epoch 338\n",
            "-------------------------------\n",
            "Training loss: 0.0983\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.942527 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.798866 \n",
            "\n",
            "Epoch 339\n",
            "-------------------------------\n",
            "Training loss: 0.0914\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 1.980579 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 1.692623 \n",
            "\n",
            "Epoch 340\n",
            "-------------------------------\n",
            "Training loss: 0.0894\n",
            "Test Error: \n",
            " Accuracy: 57.4%, Avg loss: 2.562901 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.3%, Avg loss: 2.889100 \n",
            "\n",
            "Epoch 341\n",
            "-------------------------------\n",
            "Training loss: 0.0973\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.368250 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.486269 \n",
            "\n",
            "Epoch 342\n",
            "-------------------------------\n",
            "Training loss: 0.0902\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.072040 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.049232 \n",
            "\n",
            "Epoch 343\n",
            "-------------------------------\n",
            "Training loss: 0.0884\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.628723 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 1.443525 \n",
            "\n",
            "Epoch 344\n",
            "-------------------------------\n",
            "Training loss: 0.0935\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.118745 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.092391 \n",
            "\n",
            "Epoch 345\n",
            "-------------------------------\n",
            "Training loss: 0.0853\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.444761 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.449724 \n",
            "\n",
            "Epoch 346\n",
            "-------------------------------\n",
            "Training loss: 0.0933\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.746585 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.865512 \n",
            "\n",
            "Epoch 347\n",
            "-------------------------------\n",
            "Training loss: 0.0954\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.102545 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.082724 \n",
            "\n",
            "Epoch 348\n",
            "-------------------------------\n",
            "Training loss: 0.0870\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.378769 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.349040 \n",
            "\n",
            "Epoch 349\n",
            "-------------------------------\n",
            "Training loss: 0.0919\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.077280 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.048356 \n",
            "\n",
            "Epoch 350\n",
            "-------------------------------\n",
            "Training loss: 0.0906\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.479585 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.412213 \n",
            "\n",
            "Epoch 351\n",
            "-------------------------------\n",
            "Training loss: 0.0920\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.950266 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 1.032556 \n",
            "\n",
            "Epoch 352\n",
            "-------------------------------\n",
            "Training loss: 0.0950\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.064131 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.052882 \n",
            "\n",
            "Save model: acc 0.9756838905775076, test acc 0.975609756097561, loss 0.0949737696829153\n",
            "Epoch 353\n",
            "-------------------------------\n",
            "Training loss: 0.0892\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.133807 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.082935 \n",
            "\n",
            "Epoch 354\n",
            "-------------------------------\n",
            "Training loss: 0.0924\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.105341 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.061715 \n",
            "\n",
            "Epoch 355\n",
            "-------------------------------\n",
            "Training loss: 0.0903\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.833703 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.888997 \n",
            "\n",
            "Epoch 356\n",
            "-------------------------------\n",
            "Training loss: 0.0899\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.080176 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.058634 \n",
            "\n",
            "Epoch 357\n",
            "-------------------------------\n",
            "Training loss: 0.0917\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 2.314060 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.4%, Avg loss: 1.964331 \n",
            "\n",
            "Epoch 358\n",
            "-------------------------------\n",
            "Training loss: 0.0903\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.409227 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.455508 \n",
            "\n",
            "Epoch 359\n",
            "-------------------------------\n",
            "Training loss: 0.0861\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.197408 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.172957 \n",
            "\n",
            "Epoch 360\n",
            "-------------------------------\n",
            "Training loss: 0.0934\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 1.470065 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.2%, Avg loss: 1.239931 \n",
            "\n",
            "Epoch 361\n",
            "-------------------------------\n",
            "Training loss: 0.0846\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.266321 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.211046 \n",
            "\n",
            "Epoch 362\n",
            "-------------------------------\n",
            "Training loss: 0.0837\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.243595 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.204329 \n",
            "\n",
            "Epoch 363\n",
            "-------------------------------\n",
            "Training loss: 0.0868\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.070444 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.057906 \n",
            "\n",
            "Epoch 364\n",
            "-------------------------------\n",
            "Training loss: 0.0861\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.080647 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.096687 \n",
            "\n",
            "Epoch 365\n",
            "-------------------------------\n",
            "Training loss: 0.0878\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.080359 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.066022 \n",
            "\n",
            "Epoch 366\n",
            "-------------------------------\n",
            "Training loss: 0.0884\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.205401 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.219480 \n",
            "\n",
            "Epoch 367\n",
            "-------------------------------\n",
            "Training loss: 0.0851\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.100348 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.083917 \n",
            "\n",
            "Epoch 368\n",
            "-------------------------------\n",
            "Training loss: 0.0865\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.093542 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.054428 \n",
            "\n",
            "Epoch 369\n",
            "-------------------------------\n",
            "Training loss: 0.0815\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 1.555610 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.3%, Avg loss: 1.358038 \n",
            "\n",
            "Epoch 370\n",
            "-------------------------------\n",
            "Training loss: 0.0824\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.087198 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.054617 \n",
            "\n",
            "Epoch 371\n",
            "-------------------------------\n",
            "Training loss: 0.0808\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 3.139836 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.1%, Avg loss: 3.308034 \n",
            "\n",
            "Epoch 372\n",
            "-------------------------------\n",
            "Training loss: 0.0874\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.836298 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.774057 \n",
            "\n",
            "Epoch 373\n",
            "-------------------------------\n",
            "Training loss: 0.0874\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.162400 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.157206 \n",
            "\n",
            "Epoch 374\n",
            "-------------------------------\n",
            "Training loss: 0.0844\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.098417 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.078234 \n",
            "\n",
            "Epoch 375\n",
            "-------------------------------\n",
            "Training loss: 0.0854\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.076952 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.058572 \n",
            "\n",
            "Epoch 376\n",
            "-------------------------------\n",
            "Training loss: 0.0883\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 1.089533 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.919498 \n",
            "\n",
            "Epoch 377\n",
            "-------------------------------\n",
            "Training loss: 0.0846\n",
            "Test Error: \n",
            " Accuracy: 57.4%, Avg loss: 2.009616 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 1.879519 \n",
            "\n",
            "Epoch 378\n",
            "-------------------------------\n",
            "Training loss: 0.0884\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.999715 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.874087 \n",
            "\n",
            "Epoch 379\n",
            "-------------------------------\n",
            "Training loss: 0.0924\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 7.702334 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 7.409694 \n",
            "\n",
            "Epoch 380\n",
            "-------------------------------\n",
            "Training loss: 0.1047\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.095734 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.063810 \n",
            "\n",
            "Epoch 381\n",
            "-------------------------------\n",
            "Training loss: 0.0850\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.096763 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.059678 \n",
            "\n",
            "Epoch 382\n",
            "-------------------------------\n",
            "Training loss: 0.0805\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.058028 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.040826 \n",
            "\n",
            "Epoch 383\n",
            "-------------------------------\n",
            "Training loss: 0.0852\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 4.152511 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.7%, Avg loss: 4.479122 \n",
            "\n",
            "Epoch 384\n",
            "-------------------------------\n",
            "Training loss: 0.0863\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.293468 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.332664 \n",
            "\n",
            "Epoch 385\n",
            "-------------------------------\n",
            "Training loss: 0.0848\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 2.988480 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.9%, Avg loss: 2.677079 \n",
            "\n",
            "Epoch 386\n",
            "-------------------------------\n",
            "Training loss: 0.0812\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.799714 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.716069 \n",
            "\n",
            "Epoch 387\n",
            "-------------------------------\n",
            "Training loss: 0.0806\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.225013 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 92.1%, Avg loss: 0.178930 \n",
            "\n",
            "Epoch 388\n",
            "-------------------------------\n",
            "Training loss: 0.0773\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.134711 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.105667 \n",
            "\n",
            "Epoch 389\n",
            "-------------------------------\n",
            "Training loss: 0.0803\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.361241 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.342543 \n",
            "\n",
            "Epoch 390\n",
            "-------------------------------\n",
            "Training loss: 0.0831\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.489881 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.480778 \n",
            "\n",
            "Epoch 391\n",
            "-------------------------------\n",
            "Training loss: 0.0779\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.136980 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.106271 \n",
            "\n",
            "Epoch 392\n",
            "-------------------------------\n",
            "Training loss: 0.0791\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.069438 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.052806 \n",
            "\n",
            "Epoch 393\n",
            "-------------------------------\n",
            "Training loss: 0.0835\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 1.184525 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 1.252408 \n",
            "\n",
            "Epoch 394\n",
            "-------------------------------\n",
            "Training loss: 0.0780\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.059042 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.036785 \n",
            "\n",
            "Epoch 395\n",
            "-------------------------------\n",
            "Training loss: 0.0827\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.554849 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.503545 \n",
            "\n",
            "Epoch 396\n",
            "-------------------------------\n",
            "Training loss: 0.0805\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.065982 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.049883 \n",
            "\n",
            "Epoch 397\n",
            "-------------------------------\n",
            "Training loss: 0.0807\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.112757 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.130134 \n",
            "\n",
            "Epoch 398\n",
            "-------------------------------\n",
            "Training loss: 0.0848\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.119620 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.114734 \n",
            "\n",
            "Epoch 399\n",
            "-------------------------------\n",
            "Training loss: 0.0826\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.058988 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.035151 \n",
            "\n",
            "Epoch 400\n",
            "-------------------------------\n",
            "Training loss: 0.0789\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.087535 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.057170 \n",
            "\n",
            "best valid records: acc 0.9756838905775076 test acc 0.975609756097561 loss 0.0949737696829153\n",
            "best test records: acc 0.9726443768996961 test acc 0.9878048780487805 loss 0.07800214277937058\n",
            "best train records: acc 0.9574468085106383 test acc 0.9603658536585366 loss 0.0773456946332404\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 400\n",
        "valid_records = []\n",
        "fake_test_records = []\n",
        "train_records = []\n",
        "max_valid_acc = 0\n",
        "max_test_acc = 0\n",
        "minimal_loss = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    valid_acc = test(valid_dataloader, model, loss_fn)\n",
        "    fake_test_acc = test(test_data_loader, model, loss_fn)\n",
        "    if valid_acc > max_valid_acc:\n",
        "        # save model with best valid acc\n",
        "        max_valid_acc = valid_acc\n",
        "        valid_records = [valid_acc, fake_test_acc, train_loss]\n",
        "        torch.save(model.state_dict(), \"best_vote.pt\")\n",
        "        print(f\"Save model: acc {valid_records[0]}, test acc {valid_records[1]}, loss {valid_records[2]}\")\n",
        "    if fake_test_acc > max_test_acc:\n",
        "        # record model performance with best test acc\n",
        "        max_test_acc = fake_test_acc\n",
        "        fake_test_records = [valid_acc, fake_test_acc, train_loss]\n",
        "    if train_loss < minimal_loss:\n",
        "        # record model performance with minimal loss\n",
        "        minimal_loss = train_loss\n",
        "        train_records = [valid_acc, fake_test_acc, train_loss]\n",
        "\n",
        "print(\"best valid records:\", \"acc\", valid_records[0], \"test acc\", valid_records[1], \"loss\", valid_records[2])\n",
        "print(\"best test records:\", \"acc\", fake_test_records[0], \"test acc\", fake_test_records[1], \"loss\", fake_test_records[2])\n",
        "print(\"best train records:\", \"acc\", train_records[0], \"test acc\", train_records[1], \"loss\", train_records[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_0001.npy\n",
            "test_0002.npy\n",
            "test_0003.npy\n",
            "test_0004.npy\n",
            "test_0005.npy\n",
            "test_0006.npy\n",
            "test_0007.npy\n",
            "test_0008.npy\n",
            "test_0009.npy\n",
            "test_0010.npy\n",
            "test_0011.npy\n",
            "test_0012.npy\n",
            "test_0013.npy\n",
            "test_0014.npy\n",
            "test_0015.npy\n",
            "test_0016.npy\n",
            "test_0017.npy\n",
            "test_0018.npy\n",
            "test_0019.npy\n",
            "test_0020.npy\n",
            "test_0021.npy\n",
            "test_0022.npy\n",
            "test_0023.npy\n",
            "test_0024.npy\n",
            "test_0025.npy\n",
            "test_0026.npy\n",
            "test_0027.npy\n",
            "test_0028.npy\n",
            "test_0029.npy\n",
            "test_0030.npy\n",
            "test_0031.npy\n",
            "test_0032.npy\n",
            "test_0033.npy\n",
            "test_0034.npy\n",
            "test_0035.npy\n",
            "test_0036.npy\n",
            "test_0037.npy\n",
            "test_0038.npy\n",
            "test_0039.npy\n",
            "test_0040.npy\n",
            "test_0041.npy\n",
            "test_0042.npy\n",
            "test_0043.npy\n",
            "test_0044.npy\n",
            "test_0045.npy\n",
            "test_0046.npy\n",
            "test_0047.npy\n",
            "test_0048.npy\n",
            "test_0049.npy\n",
            "test_0050.npy\n",
            "test_0051.npy\n",
            "test_0052.npy\n",
            "test_0053.npy\n",
            "test_0054.npy\n",
            "test_0055.npy\n",
            "test_0056.npy\n",
            "test_0057.npy\n",
            "test_0058.npy\n",
            "test_0059.npy\n",
            "test_0060.npy\n",
            "test_0061.npy\n",
            "test_0062.npy\n",
            "test_0063.npy\n",
            "test_0064.npy\n",
            "test_0065.npy\n",
            "test_0066.npy\n",
            "test_0067.npy\n",
            "test_0068.npy\n",
            "test_0069.npy\n",
            "test_0070.npy\n",
            "test_0071.npy\n",
            "test_0072.npy\n",
            "test_0073.npy\n",
            "test_0074.npy\n",
            "test_0075.npy\n",
            "test_0076.npy\n",
            "test_0077.npy\n",
            "test_0078.npy\n",
            "test_0079.npy\n",
            "test_0080.npy\n",
            "test_0081.npy\n",
            "test_0082.npy\n",
            "test_0083.npy\n",
            "test_0084.npy\n",
            "test_0085.npy\n",
            "test_0086.npy\n",
            "test_0087.npy\n",
            "test_0088.npy\n",
            "test_0089.npy\n",
            "test_0090.npy\n",
            "test_0091.npy\n",
            "test_0092.npy\n",
            "test_0093.npy\n",
            "test_0094.npy\n",
            "test_0095.npy\n",
            "test_0096.npy\n",
            "test_0097.npy\n",
            "test_0098.npy\n",
            "test_0099.npy\n",
            "test_0100.npy\n",
            "test_0101.npy\n",
            "test_0102.npy\n",
            "test_0103.npy\n",
            "test_0104.npy\n",
            "test_0105.npy\n",
            "test_0106.npy\n",
            "test_0107.npy\n",
            "test_0108.npy\n",
            "test_0109.npy\n",
            "test_0110.npy\n",
            "test_0111.npy\n",
            "test_0112.npy\n",
            "test_0113.npy\n",
            "test_0114.npy\n",
            "test_0115.npy\n",
            "test_0116.npy\n",
            "test_0117.npy\n",
            "test_0118.npy\n",
            "test_0119.npy\n",
            "test_0120.npy\n",
            "test_0121.npy\n",
            "test_0122.npy\n",
            "test_0123.npy\n",
            "test_0124.npy\n",
            "test_0125.npy\n",
            "test_0126.npy\n",
            "test_0127.npy\n",
            "test_0128.npy\n",
            "test_0129.npy\n",
            "test_0130.npy\n",
            "test_0131.npy\n",
            "test_0132.npy\n",
            "test_0133.npy\n",
            "test_0134.npy\n",
            "test_0135.npy\n",
            "test_0136.npy\n",
            "test_0137.npy\n",
            "test_0138.npy\n",
            "test_0139.npy\n",
            "test_0140.npy\n",
            "test_0141.npy\n",
            "test_0142.npy\n",
            "test_0143.npy\n",
            "test_0144.npy\n",
            "test_0145.npy\n",
            "test_0146.npy\n",
            "test_0147.npy\n",
            "test_0148.npy\n",
            "test_0149.npy\n",
            "test_0150.npy\n",
            "test_0151.npy\n",
            "test_0152.npy\n",
            "test_0153.npy\n",
            "test_0154.npy\n",
            "test_0155.npy\n",
            "test_0156.npy\n",
            "test_0157.npy\n",
            "test_0158.npy\n",
            "test_0159.npy\n",
            "test_0160.npy\n",
            "test_0161.npy\n",
            "test_0162.npy\n",
            "test_0163.npy\n",
            "test_0164.npy\n",
            "test_0165.npy\n",
            "test_0166.npy\n",
            "test_0167.npy\n",
            "test_0168.npy\n",
            "test_0169.npy\n",
            "test_0170.npy\n",
            "test_0171.npy\n",
            "test_0172.npy\n",
            "test_0173.npy\n",
            "test_0174.npy\n",
            "test_0175.npy\n",
            "test_0176.npy\n",
            "test_0177.npy\n",
            "test_0178.npy\n",
            "test_0179.npy\n",
            "test_0180.npy\n",
            "test_0181.npy\n",
            "test_0182.npy\n",
            "test_0183.npy\n",
            "test_0184.npy\n",
            "test_0185.npy\n",
            "test_0186.npy\n",
            "test_0187.npy\n",
            "test_0188.npy\n",
            "test_0189.npy\n",
            "test_0190.npy\n",
            "test_0191.npy\n",
            "test_0192.npy\n",
            "test_0193.npy\n",
            "test_0194.npy\n",
            "test_0195.npy\n",
            "test_0196.npy\n",
            "test_0197.npy\n",
            "test_0198.npy\n",
            "test_0199.npy\n",
            "test_0200.npy\n",
            "test_0201.npy\n",
            "test_0202.npy\n",
            "test_0203.npy\n",
            "test_0204.npy\n",
            "test_0205.npy\n",
            "test_0206.npy\n",
            "test_0207.npy\n",
            "test_0208.npy\n",
            "test_0209.npy\n",
            "test_0210.npy\n",
            "test_0211.npy\n",
            "test_0212.npy\n",
            "test_0213.npy\n",
            "test_0214.npy\n",
            "test_0215.npy\n",
            "test_0216.npy\n",
            "test_0217.npy\n",
            "test_0218.npy\n",
            "test_0219.npy\n",
            "test_0220.npy\n",
            "test_0221.npy\n",
            "test_0222.npy\n",
            "test_0223.npy\n",
            "test_0224.npy\n",
            "test_0225.npy\n",
            "test_0226.npy\n",
            "test_0227.npy\n",
            "test_0228.npy\n",
            "test_0229.npy\n",
            "test_0230.npy\n",
            "test_0231.npy\n",
            "test_0232.npy\n",
            "test_0233.npy\n",
            "test_0234.npy\n",
            "test_0235.npy\n",
            "test_0236.npy\n",
            "test_0237.npy\n",
            "test_0238.npy\n",
            "test_0239.npy\n",
            "test_0240.npy\n",
            "test_0241.npy\n",
            "test_0242.npy\n",
            "test_0243.npy\n",
            "test_0244.npy\n",
            "test_0245.npy\n",
            "test_0246.npy\n",
            "test_0247.npy\n",
            "test_0248.npy\n",
            "test_0249.npy\n",
            "test_0250.npy\n",
            "test_0251.npy\n",
            "test_0252.npy\n",
            "test_0253.npy\n",
            "test_0254.npy\n",
            "test_0255.npy\n",
            "test_0256.npy\n",
            "test_0257.npy\n",
            "test_0258.npy\n",
            "test_0259.npy\n",
            "test_0260.npy\n",
            "test_0261.npy\n",
            "test_0262.npy\n",
            "test_0263.npy\n",
            "test_0264.npy\n",
            "test_0265.npy\n",
            "test_0266.npy\n",
            "test_0267.npy\n",
            "test_0268.npy\n",
            "test_0269.npy\n",
            "test_0270.npy\n",
            "test_0271.npy\n",
            "test_0272.npy\n",
            "test_0273.npy\n",
            "test_0274.npy\n",
            "test_0275.npy\n",
            "test_0276.npy\n",
            "test_0277.npy\n",
            "test_0278.npy\n",
            "test_0279.npy\n",
            "test_0280.npy\n",
            "test_0281.npy\n",
            "test_0282.npy\n",
            "test_0283.npy\n",
            "test_0284.npy\n",
            "test_0285.npy\n",
            "test_0286.npy\n",
            "test_0287.npy\n",
            "test_0288.npy\n",
            "test_0289.npy\n",
            "test_0290.npy\n",
            "test_0291.npy\n",
            "test_0292.npy\n",
            "test_0293.npy\n",
            "test_0294.npy\n",
            "test_0295.npy\n",
            "test_0296.npy\n",
            "test_0297.npy\n",
            "test_0298.npy\n",
            "test_0299.npy\n",
            "test_0300.npy\n",
            "test_0301.npy\n",
            "test_0302.npy\n",
            "test_0303.npy\n",
            "test_0304.npy\n",
            "test_0305.npy\n",
            "test_0306.npy\n",
            "test_0307.npy\n",
            "test_0308.npy\n",
            "test_0309.npy\n",
            "test_0310.npy\n",
            "test_0311.npy\n",
            "test_0312.npy\n",
            "test_0313.npy\n",
            "test_0314.npy\n",
            "test_0315.npy\n",
            "test_0316.npy\n",
            "test_0317.npy\n",
            "test_0318.npy\n",
            "test_0319.npy\n",
            "test_0320.npy\n",
            "test_0321.npy\n",
            "test_0322.npy\n",
            "test_0323.npy\n",
            "test_0324.npy\n",
            "test_0325.npy\n",
            "test_0326.npy\n",
            "test_0327.npy\n",
            "test_0328.npy\n",
            "test_0329.npy\n",
            "test_0330.npy\n",
            "test_0331.npy\n",
            "test_0332.npy\n",
            "test_0333.npy\n",
            "test_0334.npy\n",
            "test_0335.npy\n",
            "test_0336.npy\n",
            "test_0337.npy\n",
            "test_0338.npy\n",
            "test_0339.npy\n",
            "test_0340.npy\n",
            "test_0341.npy\n",
            "test_0342.npy\n",
            "test_0343.npy\n",
            "test_0344.npy\n",
            "test_0345.npy\n",
            "test_0346.npy\n",
            "test_0347.npy\n",
            "test_0348.npy\n",
            "test_0349.npy\n",
            "test_0350.npy\n",
            "test_0351.npy\n",
            "test_0352.npy\n",
            "test_0353.npy\n",
            "test_0354.npy\n",
            "test_0355.npy\n",
            "test_0356.npy\n",
            "test_0357.npy\n",
            "test_0358.npy\n",
            "test_0359.npy\n",
            "test_0360.npy\n",
            "test_0361.npy\n",
            "test_0362.npy\n",
            "test_0363.npy\n",
            "test_0364.npy\n",
            "test_0365.npy\n",
            "test_0366.npy\n",
            "test_0367.npy\n",
            "test_0368.npy\n",
            "test_0369.npy\n",
            "test_0370.npy\n",
            "test_0371.npy\n",
            "test_0372.npy\n",
            "test_0373.npy\n",
            "test_0374.npy\n",
            "test_0375.npy\n",
            "test_0376.npy\n",
            "test_0377.npy\n",
            "test_0378.npy\n",
            "test_0379.npy\n",
            "test_0380.npy\n",
            "test_0381.npy\n",
            "test_0382.npy\n",
            "test_0383.npy\n",
            "test_0384.npy\n",
            "test_0385.npy\n",
            "test_0386.npy\n",
            "test_0387.npy\n",
            "test_0388.npy\n",
            "test_0389.npy\n",
            "test_0390.npy\n",
            "test_0391.npy\n",
            "test_0392.npy\n",
            "test_0393.npy\n",
            "test_0394.npy\n",
            "test_0395.npy\n",
            "test_0396.npy\n",
            "test_0397.npy\n",
            "test_0398.npy\n",
            "test_0399.npy\n",
            "test_0400.npy\n",
            "test_0401.npy\n",
            "test_0402.npy\n",
            "test_0403.npy\n",
            "test_0404.npy\n",
            "test_0405.npy\n",
            "test_0406.npy\n",
            "test_0407.npy\n",
            "test_0408.npy\n",
            "test_0409.npy\n",
            "test_0410.npy\n",
            "test_0411.npy\n",
            "test_0412.npy\n",
            "test_0413.npy\n",
            "test_0414.npy\n",
            "test_0415.npy\n",
            "test_0416.npy\n",
            "test_0417.npy\n",
            "test_0418.npy\n",
            "test_0419.npy\n",
            "test_0420.npy\n",
            "test_0421.npy\n",
            "test_0422.npy\n",
            "test_0423.npy\n",
            "test_0424.npy\n",
            "test_0425.npy\n",
            "test_0426.npy\n",
            "test_0427.npy\n",
            "test_0428.npy\n",
            "test_0429.npy\n",
            "test_0430.npy\n",
            "test_0431.npy\n",
            "test_0432.npy\n",
            "test_0433.npy\n",
            "test_0434.npy\n",
            "test_0435.npy\n",
            "test_0436.npy\n",
            "test_0437.npy\n",
            "test_0438.npy\n",
            "test_0439.npy\n",
            "test_0440.npy\n",
            "test_0441.npy\n",
            "test_0442.npy\n",
            "test_0443.npy\n",
            "test_0444.npy\n",
            "test_0445.npy\n",
            "test_0446.npy\n",
            "test_0447.npy\n",
            "test_0448.npy\n",
            "test_0449.npy\n",
            "test_0450.npy\n",
            "test_0451.npy\n",
            "test_0452.npy\n",
            "test_0453.npy\n",
            "test_0454.npy\n",
            "test_0455.npy\n",
            "test_0456.npy\n",
            "test_0457.npy\n",
            "test_0458.npy\n",
            "test_0459.npy\n",
            "test_0460.npy\n",
            "test_0461.npy\n",
            "test_0462.npy\n",
            "test_0463.npy\n",
            "test_0464.npy\n",
            "test_0465.npy\n",
            "test_0466.npy\n",
            "test_0467.npy\n",
            "test_0468.npy\n",
            "test_0469.npy\n",
            "test_0470.npy\n",
            "test_0471.npy\n",
            "test_0472.npy\n",
            "test_0473.npy\n",
            "test_0474.npy\n",
            "test_0475.npy\n",
            "test_0476.npy\n",
            "test_0477.npy\n",
            "test_0478.npy\n",
            "test_0479.npy\n",
            "test_0480.npy\n",
            "test_0481.npy\n",
            "test_0482.npy\n",
            "test_0483.npy\n",
            "test_0484.npy\n",
            "test_0485.npy\n",
            "test_0486.npy\n",
            "test_0487.npy\n",
            "test_0488.npy\n",
            "test_0489.npy\n",
            "test_0490.npy\n",
            "test_0491.npy\n",
            "test_0492.npy\n",
            "test_0493.npy\n",
            "test_0494.npy\n",
            "test_0495.npy\n",
            "test_0496.npy\n",
            "test_0497.npy\n",
            "test_0498.npy\n",
            "test_0499.npy\n",
            "test_0500.npy\n",
            "test_0501.npy\n",
            "test_0502.npy\n",
            "test_0503.npy\n",
            "test_0504.npy\n",
            "test_0505.npy\n",
            "test_0506.npy\n",
            "test_0507.npy\n",
            "test_0508.npy\n",
            "test_0509.npy\n",
            "test_0510.npy\n",
            "test_0511.npy\n",
            "test_0512.npy\n",
            "test_0513.npy\n",
            "test_0514.npy\n",
            "test_0515.npy\n",
            "test_0516.npy\n",
            "test_0517.npy\n",
            "test_0518.npy\n",
            "test_0519.npy\n",
            "test_0520.npy\n",
            "test_0521.npy\n",
            "test_0522.npy\n",
            "test_0523.npy\n",
            "test_0524.npy\n",
            "test_0525.npy\n",
            "test_0526.npy\n",
            "test_0527.npy\n",
            "test_0528.npy\n",
            "test_0529.npy\n",
            "test_0530.npy\n",
            "test_0531.npy\n",
            "test_0532.npy\n",
            "test_0533.npy\n",
            "test_0534.npy\n",
            "test_0535.npy\n",
            "test_0536.npy\n",
            "test_0537.npy\n",
            "test_0538.npy\n",
            "test_0539.npy\n",
            "test_0540.npy\n",
            "test_0541.npy\n",
            "test_0542.npy\n",
            "test_0543.npy\n",
            "test_0544.npy\n",
            "test_0545.npy\n",
            "test_0546.npy\n",
            "test_0547.npy\n",
            "test_0548.npy\n",
            "test_0549.npy\n",
            "test_0550.npy\n",
            "test_0551.npy\n",
            "test_0552.npy\n",
            "test_0553.npy\n",
            "test_0554.npy\n",
            "test_0555.npy\n",
            "test_0556.npy\n",
            "test_0557.npy\n",
            "test_0558.npy\n",
            "test_0559.npy\n",
            "test_0560.npy\n",
            "test_0561.npy\n",
            "test_0562.npy\n",
            "test_0563.npy\n",
            "test_0564.npy\n",
            "test_0565.npy\n",
            "test_0566.npy\n",
            "test_0567.npy\n",
            "test_0568.npy\n",
            "test_0569.npy\n",
            "test_0570.npy\n",
            "test_0571.npy\n",
            "test_0572.npy\n",
            "test_0573.npy\n",
            "test_0574.npy\n",
            "test_0575.npy\n",
            "test_0576.npy\n",
            "test_0577.npy\n",
            "test_0578.npy\n",
            "test_0579.npy\n",
            "test_0580.npy\n",
            "test_0581.npy\n",
            "test_0582.npy\n",
            "test_0583.npy\n",
            "test_0584.npy\n",
            "test_0585.npy\n",
            "test_0586.npy\n",
            "test_0587.npy\n",
            "test_0588.npy\n",
            "test_0589.npy\n",
            "test_0590.npy\n",
            "test_0591.npy\n",
            "test_0592.npy\n",
            "test_0593.npy\n",
            "test_0594.npy\n",
            "test_0595.npy\n",
            "test_0596.npy\n",
            "test_0597.npy\n",
            "test_0598.npy\n",
            "test_0599.npy\n",
            "test_0600.npy\n",
            "test_0601.npy\n",
            "test_0602.npy\n",
            "test_0603.npy\n",
            "test_0604.npy\n",
            "test_0605.npy\n",
            "test_0606.npy\n",
            "test_0607.npy\n",
            "test_0608.npy\n",
            "test_0609.npy\n",
            "test_0610.npy\n",
            "test_0611.npy\n",
            "test_0612.npy\n",
            "test_0613.npy\n",
            "test_0614.npy\n",
            "test_0615.npy\n",
            "test_0616.npy\n",
            "test_0617.npy\n",
            "test_0618.npy\n",
            "test_0619.npy\n",
            "test_0620.npy\n",
            "test_0621.npy\n",
            "test_0622.npy\n",
            "test_0623.npy\n",
            "test_0624.npy\n",
            "test_0625.npy\n",
            "test_0626.npy\n",
            "test_0627.npy\n",
            "test_0628.npy\n",
            "test_0629.npy\n",
            "test_0630.npy\n",
            "test_0631.npy\n",
            "test_0632.npy\n",
            "test_0633.npy\n",
            "test_0634.npy\n",
            "test_0635.npy\n",
            "test_0636.npy\n",
            "test_0637.npy\n",
            "test_0638.npy\n",
            "test_0639.npy\n",
            "test_0640.npy\n",
            "test_0641.npy\n",
            "test_0642.npy\n",
            "test_0643.npy\n",
            "test_0644.npy\n",
            "test_0645.npy\n",
            "test_0646.npy\n",
            "test_0647.npy\n",
            "test_0648.npy\n",
            "test_0649.npy\n",
            "test_0650.npy\n",
            "test_0651.npy\n",
            "test_0652.npy\n",
            "test_0653.npy\n",
            "test_0654.npy\n",
            "test_0655.npy\n",
            "test_0656.npy\n",
            "test_0657.npy\n",
            "test_0658.npy\n",
            "test_0659.npy\n",
            "test_0660.npy\n",
            "test_0661.npy\n",
            "test_0662.npy\n",
            "test_0663.npy\n",
            "test_0664.npy\n",
            "test_0665.npy\n",
            "test_0666.npy\n",
            "test_0667.npy\n",
            "test_0668.npy\n",
            "test_0669.npy\n",
            "test_0670.npy\n",
            "test_0671.npy\n",
            "test_0672.npy\n",
            "test_0673.npy\n",
            "test_0674.npy\n",
            "test_0675.npy\n",
            "test_0676.npy\n",
            "test_0677.npy\n",
            "test_0678.npy\n",
            "test_0679.npy\n",
            "test_0680.npy\n",
            "test_0681.npy\n",
            "test_0682.npy\n",
            "test_0683.npy\n",
            "test_0684.npy\n",
            "test_0685.npy\n",
            "test_0686.npy\n",
            "test_0687.npy\n",
            "test_0688.npy\n",
            "test_0689.npy\n",
            "test_0690.npy\n",
            "test_0691.npy\n",
            "test_0692.npy\n",
            "test_0693.npy\n",
            "test_0694.npy\n",
            "test_0695.npy\n",
            "test_0696.npy\n",
            "test_0697.npy\n",
            "test_0698.npy\n",
            "test_0699.npy\n",
            "test_0700.npy\n",
            "test_0701.npy\n",
            "test_0702.npy\n",
            "test_0703.npy\n",
            "test_0704.npy\n",
            "test_0705.npy\n",
            "test_0706.npy\n",
            "test_0707.npy\n",
            "test_0708.npy\n",
            "test_0709.npy\n",
            "test_0710.npy\n",
            "test_0711.npy\n",
            "test_0712.npy\n",
            "test_0713.npy\n",
            "test_0714.npy\n",
            "test_0715.npy\n",
            "test_0716.npy\n",
            "test_0717.npy\n",
            "test_0718.npy\n",
            "test_0719.npy\n",
            "test_0720.npy\n",
            "test_0721.npy\n",
            "test_0722.npy\n",
            "test_0723.npy\n",
            "test_0724.npy\n",
            "test_0725.npy\n",
            "test_0726.npy\n",
            "test_0727.npy\n",
            "test_0728.npy\n",
            "test_0729.npy\n",
            "test_0730.npy\n",
            "test_0731.npy\n",
            "test_0732.npy\n",
            "test_0733.npy\n",
            "test_0734.npy\n",
            "test_0735.npy\n",
            "test_0736.npy\n",
            "test_0737.npy\n",
            "test_0738.npy\n",
            "test_0739.npy\n",
            "test_0740.npy\n",
            "test_0741.npy\n",
            "test_0742.npy\n",
            "test_0743.npy\n",
            "test_0744.npy\n",
            "test_0745.npy\n",
            "test_0746.npy\n",
            "test_0747.npy\n",
            "test_0748.npy\n",
            "test_0749.npy\n",
            "test_0750.npy\n",
            "test_0751.npy\n",
            "test_0752.npy\n",
            "test_0753.npy\n",
            "test_0754.npy\n",
            "test_0755.npy\n",
            "test_0756.npy\n",
            "test_0757.npy\n",
            "test_0758.npy\n",
            "test_0759.npy\n",
            "test_0760.npy\n",
            "test_0761.npy\n",
            "test_0762.npy\n",
            "test_0763.npy\n",
            "test_0764.npy\n",
            "test_0765.npy\n",
            "test_0766.npy\n",
            "test_0767.npy\n",
            "test_0768.npy\n",
            "test_0769.npy\n",
            "test_0770.npy\n",
            "test_0771.npy\n",
            "test_0772.npy\n",
            "test_0773.npy\n",
            "test_0774.npy\n",
            "test_0775.npy\n",
            "test_0776.npy\n",
            "test_0777.npy\n",
            "test_0778.npy\n",
            "test_0779.npy\n",
            "test_0780.npy\n",
            "test_0781.npy\n",
            "test_0782.npy\n",
            "test_0783.npy\n",
            "test_0784.npy\n",
            "test_0785.npy\n",
            "test_0786.npy\n",
            "test_0787.npy\n",
            "test_0788.npy\n",
            "test_0789.npy\n",
            "test_0790.npy\n",
            "test_0791.npy\n",
            "test_0792.npy\n",
            "test_0793.npy\n",
            "test_0794.npy\n",
            "test_0795.npy\n",
            "test_0796.npy\n",
            "test_0797.npy\n",
            "test_0798.npy\n",
            "test_0799.npy\n",
            "test_0800.npy\n",
            "test_0801.npy\n",
            "test_0802.npy\n",
            "test_0803.npy\n",
            "test_0804.npy\n",
            "test_0805.npy\n",
            "test_0806.npy\n",
            "test_0807.npy\n",
            "test_0808.npy\n",
            "test_0809.npy\n",
            "test_0810.npy\n",
            "test_0811.npy\n",
            "test_0812.npy\n",
            "test_0813.npy\n",
            "test_0814.npy\n",
            "test_0815.npy\n",
            "test_0816.npy\n",
            "test_0817.npy\n",
            "test_0818.npy\n",
            "test_0819.npy\n",
            "test_0820.npy\n",
            "test_0821.npy\n",
            "test_0822.npy\n",
            "test_0823.npy\n",
            "test_0824.npy\n",
            "test_0825.npy\n",
            "test_0826.npy\n",
            "test_0827.npy\n",
            "test_0828.npy\n",
            "test_0829.npy\n",
            "test_0830.npy\n",
            "test_0831.npy\n",
            "test_0832.npy\n",
            "test_0833.npy\n",
            "test_0834.npy\n",
            "test_0835.npy\n",
            "test_0836.npy\n",
            "test_0837.npy\n",
            "test_0838.npy\n",
            "test_0839.npy\n",
            "test_0840.npy\n",
            "test_0841.npy\n",
            "test_0842.npy\n",
            "test_0843.npy\n",
            "test_0844.npy\n",
            "test_0845.npy\n",
            "test_0846.npy\n",
            "test_0847.npy\n",
            "test_0848.npy\n",
            "test_0849.npy\n",
            "test_0850.npy\n",
            "test_0851.npy\n",
            "test_0852.npy\n",
            "test_0853.npy\n",
            "test_0854.npy\n",
            "test_0855.npy\n",
            "test_0856.npy\n",
            "test_0857.npy\n",
            "test_0858.npy\n",
            "test_0859.npy\n",
            "test_0860.npy\n",
            "test_0861.npy\n",
            "test_0862.npy\n",
            "test_0863.npy\n",
            "test_0864.npy\n",
            "test_0865.npy\n",
            "test_0866.npy\n",
            "test_0867.npy\n",
            "test_0868.npy\n",
            "test_0869.npy\n",
            "test_0870.npy\n",
            "test_0871.npy\n",
            "test_0872.npy\n",
            "test_0873.npy\n",
            "test_0874.npy\n",
            "test_0875.npy\n",
            "test_0876.npy\n",
            "test_0877.npy\n",
            "test_0878.npy\n",
            "test_0879.npy\n",
            "test_0880.npy\n",
            "test_0881.npy\n",
            "test_0882.npy\n",
            "test_0883.npy\n",
            "test_0884.npy\n",
            "test_0885.npy\n",
            "test_0886.npy\n",
            "test_0887.npy\n",
            "test_0888.npy\n",
            "test_0889.npy\n",
            "test_0890.npy\n",
            "test_0891.npy\n",
            "test_0892.npy\n",
            "test_0893.npy\n",
            "test_0894.npy\n",
            "test_0895.npy\n",
            "test_0896.npy\n",
            "test_0897.npy\n",
            "test_0898.npy\n",
            "test_0899.npy\n",
            "test_0900.npy\n",
            "test_0901.npy\n",
            "test_0902.npy\n",
            "test_0903.npy\n",
            "test_0904.npy\n",
            "test_0905.npy\n",
            "test_0906.npy\n",
            "test_0907.npy\n",
            "test_0908.npy\n",
            "test_0909.npy\n",
            "test_0910.npy\n",
            "test_0911.npy\n",
            "test_0912.npy\n",
            "test_0913.npy\n",
            "test_0914.npy\n",
            "test_0915.npy\n",
            "test_0916.npy\n",
            "test_0917.npy\n",
            "test_0918.npy\n",
            "test_0919.npy\n",
            "test_0920.npy\n",
            "test_0921.npy\n",
            "test_0922.npy\n",
            "test_0923.npy\n",
            "test_0924.npy\n",
            "test_0925.npy\n",
            "test_0926.npy\n",
            "test_0927.npy\n",
            "test_0928.npy\n",
            "test_0929.npy\n",
            "test_0930.npy\n",
            "test_0931.npy\n",
            "test_0932.npy\n",
            "test_0933.npy\n",
            "test_0934.npy\n",
            "test_0935.npy\n",
            "test_0936.npy\n",
            "test_0937.npy\n",
            "test_0938.npy\n",
            "test_0939.npy\n",
            "test_0940.npy\n",
            "test_0941.npy\n",
            "test_0942.npy\n",
            "test_0943.npy\n",
            "test_0944.npy\n",
            "test_0945.npy\n",
            "test_0946.npy\n",
            "test_0947.npy\n",
            "test_0948.npy\n",
            "test_0949.npy\n",
            "test_0950.npy\n",
            "test_0951.npy\n",
            "test_0952.npy\n",
            "test_0953.npy\n",
            "test_0954.npy\n",
            "test_0955.npy\n",
            "test_0956.npy\n",
            "test_0957.npy\n",
            "test_0958.npy\n",
            "test_0959.npy\n",
            "test_0960.npy\n",
            "test_0961.npy\n",
            "test_0962.npy\n",
            "test_0963.npy\n",
            "test_0964.npy\n",
            "test_0965.npy\n",
            "test_0966.npy\n",
            "test_0967.npy\n",
            "test_0968.npy\n",
            "test_0969.npy\n",
            "test_0970.npy\n",
            "test_0971.npy\n",
            "test_0972.npy\n",
            "test_0973.npy\n",
            "test_0974.npy\n",
            "test_0975.npy\n",
            "test_0976.npy\n",
            "test_0977.npy\n",
            "test_0978.npy\n",
            "test_0979.npy\n",
            "test_0980.npy\n",
            "test_0981.npy\n",
            "test_0982.npy\n",
            "test_0983.npy\n",
            "test_0984.npy\n",
            "test_0985.npy\n",
            "test_0986.npy\n",
            "test_0987.npy\n",
            "test_0988.npy\n",
            "test_0989.npy\n",
            "test_0990.npy\n",
            "test_0991.npy\n",
            "test_0992.npy\n",
            "test_0993.npy\n",
            "test_0994.npy\n",
            "test_0995.npy\n",
            "test_0996.npy\n",
            "test_0997.npy\n",
            "test_0998.npy\n",
            "test_0999.npy\n",
            "test_1000.npy\n",
            "test_1001.npy\n",
            "test_1002.npy\n",
            "test_1003.npy\n",
            "test_1004.npy\n",
            "test_1005.npy\n",
            "test_1006.npy\n",
            "test_1007.npy\n",
            "test_1008.npy\n",
            "test_1009.npy\n",
            "test_1010.npy\n",
            "test_1011.npy\n",
            "test_1012.npy\n",
            "test_1013.npy\n",
            "test_1014.npy\n",
            "test_1015.npy\n",
            "test_1016.npy\n",
            "test_1017.npy\n",
            "test_1018.npy\n",
            "test_1019.npy\n",
            "test_1020.npy\n",
            "test_1021.npy\n",
            "test_1022.npy\n",
            "test_1023.npy\n",
            "test_1024.npy\n",
            "test_1025.npy\n",
            "test_1026.npy\n",
            "test_1027.npy\n",
            "test_1028.npy\n",
            "test_1029.npy\n",
            "test_1030.npy\n",
            "test_1031.npy\n",
            "test_1032.npy\n",
            "test_1033.npy\n",
            "test_1034.npy\n",
            "test_1035.npy\n",
            "test_1036.npy\n",
            "test_1037.npy\n",
            "test_1038.npy\n",
            "test_1039.npy\n",
            "test_1040.npy\n",
            "test_1041.npy\n",
            "test_1042.npy\n",
            "test_1043.npy\n",
            "test_1044.npy\n",
            "test_1045.npy\n",
            "test_1046.npy\n",
            "test_1047.npy\n",
            "test_1048.npy\n",
            "test_1049.npy\n",
            "test_1050.npy\n",
            "test_1051.npy\n",
            "test_1052.npy\n",
            "test_1053.npy\n",
            "test_1054.npy\n",
            "test_1055.npy\n",
            "test_1056.npy\n",
            "test_1057.npy\n",
            "test_1058.npy\n",
            "test_1059.npy\n",
            "test_1060.npy\n",
            "test_1061.npy\n",
            "test_1062.npy\n",
            "test_1063.npy\n",
            "test_1064.npy\n",
            "test_1065.npy\n",
            "test_1066.npy\n",
            "test_1067.npy\n",
            "test_1068.npy\n",
            "test_1069.npy\n",
            "test_1070.npy\n",
            "test_1071.npy\n",
            "test_1072.npy\n",
            "test_1073.npy\n",
            "test_1074.npy\n",
            "test_1075.npy\n",
            "test_1076.npy\n",
            "test_1077.npy\n",
            "test_1078.npy\n",
            "test_1079.npy\n",
            "test_1080.npy\n",
            "test_1081.npy\n",
            "test_1082.npy\n",
            "test_1083.npy\n",
            "test_1084.npy\n",
            "test_1085.npy\n",
            "test_1086.npy\n",
            "test_1087.npy\n",
            "test_1088.npy\n",
            "test_1089.npy\n",
            "test_1090.npy\n",
            "test_1091.npy\n",
            "test_1092.npy\n",
            "test_1093.npy\n",
            "test_1094.npy\n",
            "test_1095.npy\n",
            "test_1096.npy\n",
            "test_1097.npy\n",
            "test_1098.npy\n",
            "test_1099.npy\n",
            "test_1100.npy\n",
            "test_1101.npy\n",
            "test_1102.npy\n",
            "test_1103.npy\n",
            "test_1104.npy\n",
            "test_1105.npy\n",
            "test_1106.npy\n",
            "test_1107.npy\n",
            "test_1108.npy\n",
            "test_1109.npy\n",
            "test_1110.npy\n",
            "test_1111.npy\n",
            "test_1112.npy\n",
            "test_1113.npy\n",
            "test_1114.npy\n",
            "test_1115.npy\n",
            "test_1116.npy\n",
            "test_1117.npy\n",
            "test_1118.npy\n",
            "test_1119.npy\n",
            "test_1120.npy\n",
            "test_1121.npy\n",
            "test_1122.npy\n",
            "test_1123.npy\n",
            "test_1124.npy\n",
            "test_1125.npy\n",
            "test_1126.npy\n",
            "test_1127.npy\n",
            "test_1128.npy\n",
            "test_1129.npy\n",
            "test_1130.npy\n",
            "test_1131.npy\n",
            "test_1132.npy\n",
            "test_1133.npy\n",
            "test_1134.npy\n",
            "test_1135.npy\n",
            "test_1136.npy\n",
            "test_1137.npy\n",
            "test_1138.npy\n",
            "test_1139.npy\n",
            "test_1140.npy\n",
            "test_1141.npy\n",
            "test_1142.npy\n",
            "test_1143.npy\n",
            "test_1144.npy\n",
            "test_1145.npy\n",
            "test_1146.npy\n",
            "test_1147.npy\n",
            "test_1148.npy\n",
            "test_1149.npy\n",
            "test_1150.npy\n",
            "test_1151.npy\n",
            "test_1152.npy\n",
            "test_1153.npy\n",
            "test_1154.npy\n",
            "test_1155.npy\n",
            "test_1156.npy\n",
            "test_1157.npy\n",
            "test_1158.npy\n",
            "test_1159.npy\n",
            "test_1160.npy\n",
            "test_1161.npy\n",
            "test_1162.npy\n",
            "test_1163.npy\n",
            "test_1164.npy\n",
            "test_1165.npy\n",
            "test_1166.npy\n",
            "test_1167.npy\n",
            "test_1168.npy\n",
            "test_1169.npy\n",
            "test_1170.npy\n",
            "test_1171.npy\n",
            "test_1172.npy\n",
            "test_1173.npy\n",
            "test_1174.npy\n",
            "test_1175.npy\n",
            "test_1176.npy\n",
            "test_1177.npy\n",
            "test_1178.npy\n",
            "test_1179.npy\n",
            "test_1180.npy\n",
            "test_1181.npy\n",
            "test_1182.npy\n",
            "test_1183.npy\n",
            "test_1184.npy\n",
            "test_1185.npy\n",
            "test_1186.npy\n",
            "test_1187.npy\n",
            "test_1188.npy\n",
            "test_1189.npy\n",
            "test_1190.npy\n",
            "test_1191.npy\n",
            "test_1192.npy\n",
            "test_1193.npy\n",
            "test_1194.npy\n",
            "test_1195.npy\n",
            "test_1196.npy\n",
            "test_1197.npy\n",
            "test_1198.npy\n",
            "test_1199.npy\n",
            "test_1200.npy\n",
            "test_1201.npy\n",
            "test_1202.npy\n",
            "test_1203.npy\n",
            "test_1204.npy\n",
            "test_1205.npy\n",
            "test_1206.npy\n",
            "test_1207.npy\n",
            "test_1208.npy\n",
            "test_1209.npy\n",
            "test_1210.npy\n",
            "test_1211.npy\n",
            "test_1212.npy\n",
            "test_1213.npy\n",
            "test_1214.npy\n",
            "test_1215.npy\n",
            "test_1216.npy\n",
            "test_1217.npy\n",
            "test_1218.npy\n",
            "test_1219.npy\n",
            "test_1220.npy\n",
            "test_1221.npy\n",
            "test_1222.npy\n",
            "test_1223.npy\n",
            "test_1224.npy\n",
            "test_1225.npy\n",
            "test_1226.npy\n",
            "test_1227.npy\n",
            "test_1228.npy\n",
            "test_1229.npy\n",
            "test_1230.npy\n",
            "test_1231.npy\n",
            "test_1232.npy\n",
            "test_1233.npy\n",
            "test_1234.npy\n",
            "test_1235.npy\n",
            "test_1236.npy\n",
            "test_1237.npy\n",
            "test_1238.npy\n",
            "test_1239.npy\n",
            "test_1240.npy\n",
            "test_1241.npy\n",
            "test_1242.npy\n",
            "test_1243.npy\n",
            "test_1244.npy\n",
            "test_1245.npy\n",
            "test_1246.npy\n",
            "test_1247.npy\n",
            "test_1248.npy\n",
            "test_1249.npy\n",
            "test_1250.npy\n",
            "test_1251.npy\n",
            "test_1252.npy\n",
            "test_1253.npy\n",
            "test_1254.npy\n",
            "test_1255.npy\n",
            "test_1256.npy\n",
            "test_1257.npy\n",
            "test_1258.npy\n",
            "test_1259.npy\n",
            "test_1260.npy\n",
            "test_1261.npy\n",
            "test_1262.npy\n",
            "test_1263.npy\n",
            "test_1264.npy\n",
            "test_1265.npy\n",
            "test_1266.npy\n",
            "test_1267.npy\n",
            "test_1268.npy\n",
            "test_1269.npy\n",
            "test_1270.npy\n",
            "test_1271.npy\n",
            "test_1272.npy\n",
            "test_1273.npy\n",
            "test_1274.npy\n",
            "test_1275.npy\n",
            "test_1276.npy\n",
            "test_1277.npy\n",
            "test_1278.npy\n",
            "test_1279.npy\n",
            "test_1280.npy\n",
            "test_1281.npy\n",
            "test_1282.npy\n",
            "test_1283.npy\n",
            "test_1284.npy\n",
            "test_1285.npy\n",
            "test_1286.npy\n",
            "test_1287.npy\n",
            "test_1288.npy\n",
            "test_1289.npy\n",
            "test_1290.npy\n",
            "test_1291.npy\n",
            "test_1292.npy\n",
            "test_1293.npy\n",
            "test_1294.npy\n",
            "test_1295.npy\n",
            "test_1296.npy\n",
            "test_1297.npy\n",
            "test_1298.npy\n",
            "test_1299.npy\n",
            "test_1300.npy\n",
            "test_1301.npy\n",
            "test_1302.npy\n",
            "test_1303.npy\n",
            "test_1304.npy\n",
            "test_1305.npy\n",
            "test_1306.npy\n",
            "test_1307.npy\n",
            "test_1308.npy\n",
            "test_1309.npy\n",
            "test_1310.npy\n",
            "test_1311.npy\n",
            "test_1312.npy\n",
            "test_1313.npy\n",
            "test_1314.npy\n",
            "test_1315.npy\n",
            "test_1316.npy\n",
            "test_1317.npy\n",
            "test_1318.npy\n",
            "test_1319.npy\n",
            "test_1320.npy\n",
            "test_1321.npy\n",
            "test_1322.npy\n",
            "test_1323.npy\n",
            "test_1324.npy\n",
            "test_1325.npy\n",
            "test_1326.npy\n",
            "test_1327.npy\n",
            "test_1328.npy\n",
            "test_1329.npy\n",
            "test_1330.npy\n",
            "test_1331.npy\n",
            "test_1332.npy\n",
            "test_1333.npy\n",
            "test_1334.npy\n",
            "test_1335.npy\n",
            "test_1336.npy\n",
            "test_1337.npy\n",
            "test_1338.npy\n",
            "test_1339.npy\n",
            "test_1340.npy\n",
            "test_1341.npy\n",
            "test_1342.npy\n",
            "test_1343.npy\n",
            "test_1344.npy\n",
            "test_1345.npy\n",
            "test_1346.npy\n",
            "test_1347.npy\n",
            "test_1348.npy\n",
            "test_1349.npy\n",
            "test_1350.npy\n",
            "test_1351.npy\n",
            "test_1352.npy\n",
            "test_1353.npy\n",
            "test_1354.npy\n",
            "test_1355.npy\n",
            "test_1356.npy\n",
            "test_1357.npy\n",
            "test_1358.npy\n",
            "test_1359.npy\n",
            "test_1360.npy\n",
            "test_1361.npy\n",
            "test_1362.npy\n",
            "test_1363.npy\n",
            "test_1364.npy\n",
            "test_1365.npy\n",
            "test_1366.npy\n",
            "test_1367.npy\n",
            "test_1368.npy\n",
            "test_1369.npy\n",
            "test_1370.npy\n",
            "test_1371.npy\n",
            "test_1372.npy\n",
            "test_1373.npy\n",
            "test_1374.npy\n",
            "test_1375.npy\n",
            "test_1376.npy\n",
            "test_1377.npy\n",
            "test_1378.npy\n",
            "test_1379.npy\n",
            "test_1380.npy\n",
            "test_1381.npy\n",
            "test_1382.npy\n",
            "test_1383.npy\n",
            "test_1384.npy\n",
            "test_1385.npy\n",
            "test_1386.npy\n",
            "test_1387.npy\n",
            "test_1388.npy\n",
            "test_1389.npy\n",
            "test_1390.npy\n",
            "test_1391.npy\n",
            "test_1392.npy\n",
            "test_1393.npy\n",
            "test_1394.npy\n",
            "test_1395.npy\n",
            "test_1396.npy\n",
            "test_1397.npy\n",
            "test_1398.npy\n",
            "test_1399.npy\n",
            "test_1400.npy\n",
            "test_1401.npy\n",
            "test_1402.npy\n",
            "test_1403.npy\n",
            "test_1404.npy\n",
            "test_1405.npy\n",
            "test_1406.npy\n",
            "test_1407.npy\n",
            "test_1408.npy\n",
            "test_1409.npy\n",
            "test_1410.npy\n",
            "test_1411.npy\n",
            "test_1412.npy\n",
            "test_1413.npy\n",
            "test_1414.npy\n",
            "test_1415.npy\n",
            "test_1416.npy\n",
            "test_1417.npy\n",
            "test_1418.npy\n",
            "test_1419.npy\n",
            "test_1420.npy\n",
            "test_1421.npy\n",
            "test_1422.npy\n",
            "test_1423.npy\n",
            "test_1424.npy\n",
            "test_1425.npy\n",
            "test_1426.npy\n",
            "test_1427.npy\n",
            "test_1428.npy\n",
            "test_1429.npy\n",
            "test_1430.npy\n",
            "test_1431.npy\n",
            "test_1432.npy\n",
            "test_1433.npy\n",
            "test_1434.npy\n",
            "test_1435.npy\n",
            "test_1436.npy\n",
            "test_1437.npy\n",
            "test_1438.npy\n",
            "test_1439.npy\n",
            "test_1440.npy\n",
            "test_1441.npy\n",
            "test_1442.npy\n",
            "test_1443.npy\n",
            "test_1444.npy\n",
            "test_1445.npy\n",
            "test_1446.npy\n",
            "test_1447.npy\n",
            "test_1448.npy\n",
            "test_1449.npy\n",
            "test_1450.npy\n",
            "test_1451.npy\n",
            "test_1452.npy\n",
            "test_1453.npy\n",
            "test_1454.npy\n",
            "test_1455.npy\n",
            "test_1456.npy\n",
            "test_1457.npy\n",
            "test_1458.npy\n",
            "test_1459.npy\n",
            "test_1460.npy\n",
            "test_1461.npy\n",
            "test_1462.npy\n",
            "test_1463.npy\n",
            "test_1464.npy\n",
            "test_1465.npy\n",
            "test_1466.npy\n",
            "test_1467.npy\n",
            "test_1468.npy\n",
            "test_1469.npy\n",
            "test_1470.npy\n",
            "test_1471.npy\n",
            "test_1472.npy\n",
            "test_1473.npy\n",
            "test_1474.npy\n",
            "test_1475.npy\n",
            "test_1476.npy\n",
            "test_1477.npy\n",
            "test_1478.npy\n",
            "test_1479.npy\n",
            "test_1480.npy\n",
            "test_1481.npy\n",
            "test_1482.npy\n",
            "test_1483.npy\n",
            "test_1484.npy\n",
            "test_1485.npy\n",
            "test_1486.npy\n",
            "test_1487.npy\n",
            "test_1488.npy\n",
            "test_1489.npy\n",
            "test_1490.npy\n",
            "test_1491.npy\n",
            "test_1492.npy\n",
            "test_1493.npy\n",
            "test_1494.npy\n",
            "test_1495.npy\n",
            "test_1496.npy\n",
            "test_1497.npy\n",
            "test_1498.npy\n",
            "test_1499.npy\n",
            "test_1500.npy\n",
            "test_1501.npy\n",
            "test_1502.npy\n",
            "test_1503.npy\n",
            "test_1504.npy\n",
            "test_1505.npy\n",
            "test_1506.npy\n",
            "test_1507.npy\n",
            "test_1508.npy\n",
            "test_1509.npy\n",
            "test_1510.npy\n",
            "test_1511.npy\n",
            "test_1512.npy\n",
            "test_1513.npy\n",
            "test_1514.npy\n",
            "test_1515.npy\n",
            "test_1516.npy\n",
            "test_1517.npy\n",
            "test_1518.npy\n",
            "test_1519.npy\n",
            "test_1520.npy\n",
            "test_1521.npy\n",
            "test_1522.npy\n",
            "test_1523.npy\n",
            "test_1524.npy\n",
            "test_1525.npy\n",
            "test_1526.npy\n",
            "test_1527.npy\n",
            "test_1528.npy\n",
            "test_1529.npy\n",
            "test_1530.npy\n",
            "test_1531.npy\n",
            "test_1532.npy\n",
            "test_1533.npy\n",
            "test_1534.npy\n",
            "test_1535.npy\n",
            "test_1536.npy\n",
            "test_1537.npy\n",
            "test_1538.npy\n",
            "test_1539.npy\n",
            "test_1540.npy\n",
            "test_1541.npy\n",
            "test_1542.npy\n",
            "test_1543.npy\n",
            "test_1544.npy\n",
            "test_1545.npy\n",
            "test_1546.npy\n",
            "test_1547.npy\n",
            "test_1548.npy\n",
            "test_1549.npy\n",
            "test_1550.npy\n",
            "test_1551.npy\n",
            "test_1552.npy\n",
            "test_1553.npy\n",
            "test_1554.npy\n",
            "test_1555.npy\n",
            "test_1556.npy\n",
            "test_1557.npy\n",
            "test_1558.npy\n",
            "test_1559.npy\n",
            "test_1560.npy\n",
            "test_1561.npy\n",
            "test_1562.npy\n",
            "test_1563.npy\n",
            "test_1564.npy\n",
            "test_1565.npy\n",
            "test_1566.npy\n",
            "test_1567.npy\n",
            "test_1568.npy\n",
            "test_1569.npy\n",
            "test_1570.npy\n",
            "test_1571.npy\n",
            "test_1572.npy\n",
            "test_1573.npy\n",
            "test_1574.npy\n",
            "test_1575.npy\n",
            "test_1576.npy\n",
            "test_1577.npy\n",
            "test_1578.npy\n",
            "test_1579.npy\n",
            "test_1580.npy\n",
            "test_1581.npy\n",
            "test_1582.npy\n",
            "test_1583.npy\n",
            "test_1584.npy\n",
            "test_1585.npy\n",
            "test_1586.npy\n",
            "test_1587.npy\n",
            "test_1588.npy\n",
            "test_1589.npy\n",
            "test_1590.npy\n",
            "test_1591.npy\n",
            "test_1592.npy\n",
            "test_1593.npy\n",
            "test_1594.npy\n",
            "test_1595.npy\n",
            "test_1596.npy\n",
            "test_1597.npy\n",
            "test_1598.npy\n",
            "test_1599.npy\n",
            "test_1600.npy\n",
            "test_1601.npy\n",
            "test_1602.npy\n",
            "test_1603.npy\n",
            "test_1604.npy\n",
            "test_1605.npy\n",
            "test_1606.npy\n",
            "test_1607.npy\n",
            "test_1608.npy\n",
            "test_1609.npy\n",
            "test_1610.npy\n",
            "test_1611.npy\n",
            "test_1612.npy\n",
            "test_1613.npy\n",
            "test_1614.npy\n",
            "test_1615.npy\n",
            "test_1616.npy\n",
            "test_1617.npy\n",
            "test_1618.npy\n",
            "test_1619.npy\n",
            "test_1620.npy\n",
            "test_1621.npy\n",
            "test_1622.npy\n",
            "test_1623.npy\n",
            "test_1624.npy\n",
            "test_1625.npy\n",
            "test_1626.npy\n",
            "test_1627.npy\n",
            "test_1628.npy\n",
            "test_1629.npy\n",
            "test_1630.npy\n",
            "test_1631.npy\n",
            "test_1632.npy\n",
            "test_1633.npy\n",
            "test_1634.npy\n",
            "test_1635.npy\n",
            "test_1636.npy\n",
            "test_1637.npy\n",
            "test_1638.npy\n",
            "test_1639.npy\n",
            "test_1640.npy\n",
            "test_1641.npy\n",
            "test_1642.npy\n",
            "test_1643.npy\n",
            "test_1644.npy\n",
            "test_1645.npy\n",
            "test_1646.npy\n",
            "test_1647.npy\n",
            "test_1648.npy\n",
            "test_1649.npy\n",
            "test_1650.npy\n",
            "test_1651.npy\n",
            "test_1652.npy\n",
            "test_1653.npy\n",
            "test_1654.npy\n",
            "test_1655.npy\n",
            "test_1656.npy\n",
            "test_1657.npy\n",
            "test_1658.npy\n",
            "test_1659.npy\n",
            "test_1660.npy\n",
            "test_1661.npy\n",
            "test_1662.npy\n",
            "test_1663.npy\n",
            "test_1664.npy\n",
            "test_1665.npy\n",
            "test_1666.npy\n",
            "test_1667.npy\n",
            "test_1668.npy\n",
            "test_1669.npy\n",
            "test_1670.npy\n",
            "test_1671.npy\n",
            "test_1672.npy\n",
            "test_1673.npy\n",
            "test_1674.npy\n",
            "test_1675.npy\n",
            "test_1676.npy\n",
            "test_1677.npy\n",
            "test_1678.npy\n",
            "test_1679.npy\n",
            "test_1680.npy\n",
            "test_1681.npy\n",
            "test_1682.npy\n",
            "test_1683.npy\n",
            "test_1684.npy\n",
            "test_1685.npy\n",
            "test_1686.npy\n",
            "test_1687.npy\n",
            "test_1688.npy\n",
            "test_1689.npy\n",
            "test_1690.npy\n",
            "test_1691.npy\n",
            "test_1692.npy\n",
            "test_1693.npy\n",
            "test_1694.npy\n",
            "test_1695.npy\n",
            "test_1696.npy\n",
            "test_1697.npy\n",
            "test_1698.npy\n",
            "test_1699.npy\n",
            "test_1700.npy\n",
            "test_1701.npy\n",
            "test_1702.npy\n",
            "test_1703.npy\n",
            "test_1704.npy\n",
            "test_1705.npy\n",
            "test_1706.npy\n",
            "test_1707.npy\n",
            "test_1708.npy\n",
            "test_1709.npy\n",
            "test_1710.npy\n",
            "test_1711.npy\n",
            "test_1712.npy\n",
            "test_1713.npy\n",
            "test_1714.npy\n",
            "test_1715.npy\n",
            "test_1716.npy\n",
            "test_1717.npy\n",
            "test_1718.npy\n",
            "test_1719.npy\n",
            "test_1720.npy\n",
            "test_1721.npy\n",
            "test_1722.npy\n",
            "test_1723.npy\n",
            "test_1724.npy\n",
            "test_1725.npy\n",
            "test_1726.npy\n",
            "test_1727.npy\n",
            "test_1728.npy\n",
            "test_1729.npy\n",
            "test_1730.npy\n",
            "test_1731.npy\n",
            "test_1732.npy\n",
            "test_1733.npy\n",
            "test_1734.npy\n",
            "test_1735.npy\n",
            "test_1736.npy\n",
            "test_1737.npy\n",
            "test_1738.npy\n",
            "test_1739.npy\n",
            "test_1740.npy\n",
            "test_1741.npy\n",
            "test_1742.npy\n",
            "test_1743.npy\n",
            "test_1744.npy\n",
            "test_1745.npy\n",
            "test_1746.npy\n",
            "test_1747.npy\n",
            "test_1748.npy\n",
            "test_1749.npy\n",
            "test_1750.npy\n",
            "test_1751.npy\n",
            "test_1752.npy\n",
            "test_1753.npy\n",
            "test_1754.npy\n",
            "test_1755.npy\n",
            "test_1756.npy\n",
            "test_1757.npy\n",
            "test_1758.npy\n",
            "test_1759.npy\n",
            "test_1760.npy\n",
            "test_1761.npy\n",
            "test_1762.npy\n",
            "test_1763.npy\n",
            "test_1764.npy\n",
            "test_1765.npy\n",
            "test_1766.npy\n",
            "test_1767.npy\n",
            "test_1768.npy\n",
            "test_1769.npy\n",
            "test_1770.npy\n",
            "test_1771.npy\n",
            "test_1772.npy\n",
            "test_1773.npy\n",
            "test_1774.npy\n",
            "test_1775.npy\n",
            "test_1776.npy\n",
            "test_1777.npy\n",
            "test_1778.npy\n",
            "test_1779.npy\n",
            "test_1780.npy\n",
            "test_1781.npy\n",
            "test_1782.npy\n",
            "test_1783.npy\n",
            "test_1784.npy\n",
            "test_1785.npy\n",
            "test_1786.npy\n",
            "test_1787.npy\n",
            "test_1788.npy\n",
            "test_1789.npy\n",
            "test_1790.npy\n",
            "test_1791.npy\n",
            "test_1792.npy\n",
            "test_1793.npy\n",
            "test_1794.npy\n",
            "test_1795.npy\n",
            "test_1796.npy\n",
            "test_1797.npy\n",
            "test_1798.npy\n",
            "test_1799.npy\n",
            "test_1800.npy\n",
            "test_1801.npy\n",
            "test_1802.npy\n",
            "test_1803.npy\n",
            "test_1804.npy\n",
            "test_1805.npy\n",
            "test_1806.npy\n",
            "test_1807.npy\n",
            "test_1808.npy\n",
            "test_1809.npy\n",
            "test_1810.npy\n",
            "test_1811.npy\n",
            "test_1812.npy\n",
            "test_1813.npy\n",
            "test_1814.npy\n",
            "test_1815.npy\n",
            "test_1816.npy\n",
            "test_1817.npy\n",
            "test_1818.npy\n",
            "test_1819.npy\n",
            "test_1820.npy\n",
            "test_1821.npy\n",
            "test_1822.npy\n",
            "test_1823.npy\n",
            "test_1824.npy\n",
            "test_1825.npy\n",
            "test_1826.npy\n",
            "test_1827.npy\n",
            "test_1828.npy\n",
            "test_1829.npy\n",
            "test_1830.npy\n",
            "test_1831.npy\n",
            "test_1832.npy\n",
            "test_1833.npy\n",
            "test_1834.npy\n",
            "test_1835.npy\n",
            "test_1836.npy\n",
            "test_1837.npy\n",
            "test_1838.npy\n",
            "test_1839.npy\n",
            "test_1840.npy\n",
            "test_1841.npy\n",
            "test_1842.npy\n",
            "test_1843.npy\n",
            "test_1844.npy\n",
            "test_1845.npy\n",
            "test_1846.npy\n",
            "test_1847.npy\n",
            "test_1848.npy\n",
            "test_1849.npy\n",
            "test_1850.npy\n",
            "test_1851.npy\n",
            "test_1852.npy\n",
            "test_1853.npy\n",
            "test_1854.npy\n",
            "test_1855.npy\n",
            "test_1856.npy\n",
            "test_1857.npy\n",
            "test_1858.npy\n",
            "test_1859.npy\n",
            "test_1860.npy\n",
            "test_1861.npy\n",
            "test_1862.npy\n",
            "test_1863.npy\n",
            "test_1864.npy\n",
            "test_1865.npy\n",
            "test_1866.npy\n",
            "test_1867.npy\n",
            "test_1868.npy\n",
            "test_1869.npy\n",
            "test_1870.npy\n",
            "test_1871.npy\n",
            "test_1872.npy\n",
            "test_1873.npy\n",
            "test_1874.npy\n",
            "test_1875.npy\n",
            "test_1876.npy\n",
            "test_1877.npy\n",
            "test_1878.npy\n",
            "test_1879.npy\n",
            "test_1880.npy\n",
            "test_1881.npy\n",
            "test_1882.npy\n",
            "test_1883.npy\n",
            "test_1884.npy\n",
            "test_1885.npy\n",
            "test_1886.npy\n",
            "test_1887.npy\n",
            "test_1888.npy\n",
            "test_1889.npy\n",
            "test_1890.npy\n",
            "test_1891.npy\n",
            "test_1892.npy\n",
            "test_1893.npy\n",
            "test_1894.npy\n",
            "test_1895.npy\n",
            "test_1896.npy\n",
            "test_1897.npy\n",
            "test_1898.npy\n",
            "test_1899.npy\n",
            "test_1900.npy\n",
            "test_1901.npy\n",
            "test_1902.npy\n",
            "test_1903.npy\n",
            "test_1904.npy\n",
            "test_1905.npy\n",
            "test_1906.npy\n",
            "test_1907.npy\n",
            "test_1908.npy\n",
            "test_1909.npy\n",
            "test_1910.npy\n",
            "test_1911.npy\n",
            "test_1912.npy\n",
            "test_1913.npy\n",
            "test_1914.npy\n",
            "test_1915.npy\n",
            "test_1916.npy\n",
            "test_1917.npy\n",
            "test_1918.npy\n",
            "test_1919.npy\n",
            "test_1920.npy\n",
            "test_1921.npy\n",
            "test_1922.npy\n",
            "test_1923.npy\n",
            "test_1924.npy\n",
            "test_1925.npy\n",
            "test_1926.npy\n",
            "test_1927.npy\n",
            "test_1928.npy\n",
            "test_1929.npy\n",
            "test_1930.npy\n",
            "test_1931.npy\n",
            "test_1932.npy\n",
            "test_1933.npy\n",
            "test_1934.npy\n",
            "test_1935.npy\n",
            "test_1936.npy\n",
            "test_1937.npy\n",
            "test_1938.npy\n",
            "test_1939.npy\n",
            "test_1940.npy\n",
            "test_1941.npy\n",
            "test_1942.npy\n",
            "test_1943.npy\n",
            "test_1944.npy\n",
            "test_1945.npy\n",
            "test_1946.npy\n",
            "test_1947.npy\n",
            "test_1948.npy\n",
            "test_1949.npy\n",
            "test_1950.npy\n",
            "test_1951.npy\n",
            "test_1952.npy\n",
            "test_1953.npy\n",
            "test_1954.npy\n",
            "test_1955.npy\n",
            "test_1956.npy\n",
            "test_1957.npy\n",
            "test_1958.npy\n",
            "test_1959.npy\n",
            "test_1960.npy\n",
            "test_1961.npy\n",
            "test_1962.npy\n",
            "test_1963.npy\n",
            "test_1964.npy\n",
            "test_1965.npy\n",
            "test_1966.npy\n",
            "test_1967.npy\n",
            "test_1968.npy\n",
            "test_1969.npy\n",
            "test_1970.npy\n",
            "test_1971.npy\n",
            "test_1972.npy\n",
            "test_1973.npy\n",
            "test_1974.npy\n",
            "test_1975.npy\n",
            "test_1976.npy\n",
            "test_1977.npy\n",
            "test_1978.npy\n",
            "test_1979.npy\n",
            "test_1980.npy\n",
            "test_1981.npy\n",
            "test_1982.npy\n",
            "test_1983.npy\n",
            "test_1984.npy\n",
            "test_1985.npy\n",
            "test_1986.npy\n",
            "test_1987.npy\n",
            "test_1988.npy\n",
            "test_1989.npy\n",
            "test_1990.npy\n",
            "test_1991.npy\n",
            "test_1992.npy\n",
            "test_1993.npy\n",
            "test_1994.npy\n",
            "test_1995.npy\n",
            "test_1996.npy\n",
            "test_1997.npy\n",
            "test_1998.npy\n",
            "test_1999.npy\n",
            "test_2000.npy\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"best_vote.pt\"))\n",
        "\n",
        "test_X = []\n",
        "# load test data\n",
        "listed_files = os.listdir(test_data_path)\n",
        "# sort the file names so that the order of test_X is the same as those in the test.csv\n",
        "listed_files.sort()\n",
        "\n",
        "for file in listed_files:\n",
        "    print(file)\n",
        "    mel_data = np.load(os.path.join(test_data_path, file))\n",
        "    data_splits = split_long_mel(mel_data, clip_target)\n",
        "    test_X.append(data_splits[0])\n",
        "test_X = np.array(test_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model can now be used to make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 1, 256, 80])\n",
            "[0 0 0 ... 0 1 0]\n",
            "2000\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor(test_X)\n",
        "    if len(test_X.shape) == 3:\n",
        "        test_X = test_X.unsqueeze(1)\n",
        "    print(test_X.shape)\n",
        "    test_X = test_X.to(device)\n",
        "    pred = model(test_X)\n",
        "    predicted = pred.argmax(1)\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    print(predicted)\n",
        "    print(len(predicted))\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"./test.csv\")\n",
        "# replace the label column with predicted labels\n",
        "df[\"label\"] = predicted\n",
        "df.to_csv(\"./test.csv\", index=False)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
